---
uuid: ff0284e0-7a8c-11ef-a022-4da48f857b44
title: '论文阅读:离线强化学习中的扩散策略'
author: Ameshiro
top: false
cover: 'https://s2.loli.net/2023/04/03/pvexKZFJ94oGbu8.jpg'
toc: true
mathjax: true
copyright_author: Ameshiro
hidden: false
tags:
  - 扩散模型
  - 生成式AI
  - 强化学习
  - 论文阅读
categories:
  - 强化学习
abbrlink: 1c6e6ef6
date: 2024-09-20 23:52:26
updated:
description:
img:
top_img:
password:
summary:
copyright:
copyright_author_href:
copyright_url:
copyright_info:
aplayer:
highlight_shrink:
aside:
---

论文地址：[[2208.06193\] Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning (arxiv.org)](https://arxiv.org/abs/2208.06193#:~:text=A paper that proposes using diffusion)

# 摘要

​	离线强化学习旨在使用**先前收集的静态数据集**学习最优策略。标准的RL方法通常在这种情况下表现不佳，因为存在对于**分布外动作（不满足于数据集分布）**的函数近似误差。虽然已经提出了多种正则化方法来缓解这个问题，但它们通常受到表达能力有限的策略类别的限制。本文提出将策略表示为**扩散模型**，引入了扩散Q学习（Diffusion-QL,DQL），利用条件扩散模型来表示策略（条件即观测的状态）。本文学习一个动作价值函数，并且在条件扩散模型的训练损失中添加了一个最大化动作价值的项，使得损失寻求在behavior policy附近的最佳动作（即模仿学习，给出的数据集通常是expert dataset）。我们展示了基于扩散模型的策略的表达能力，以及在扩散模型下的行为克隆和策略改进的耦合，都有助于Diffusion-QL的卓越性能。

# 简介

​	离线强化学习旨在完全从先前收集的数据中学习有效的策略，而无需与环境交互。消除与环境在线交互的需求使得离线RL对于自动驾驶这种应用很有利，因为使用未训练的策略进行现实世界探索是风险高、成本高或耗时的。离线RL强调使用先前的数据（如人类演示），这些数据通常比在线交互的成本要低得多。然而，仅依赖先前收集的数据使得离线RL成为一个具有挑战性的任务，**因为标准策略改进方法应用于离线数据集通常会导致模型依赖于评估在数据集中未见过的动作（即：没有用改进后的策略执行的action）**，因此它们的价值不太可能被准确估计。因此，对于离线RL的简单方法通常学习到的策略性能不佳，**倾向于偏好分布外动作**，其价值被高估，导致不满意的性能。

​	先前关于离线RL的工作通常以四种方式之一解决这个问题：1.通过**正则化**来限制策略与行为策略的偏离程度；2.限制学习到的价值函数，使其对分布外动作赋予低价值；3.引入基于模型的方法（**就是学习环境，现在基本都是model-free的，不用学习环境**）；4.将离线RL视为带有reward指导的序列预测问题。**本文分方法是第一类。**从经验上看，策略正则化的离线RL方法的性能通常略逊于其他方法，这主要是因为策略正则化方法由于其有限的能力来准确表示行为策略，因此表现不佳。例如，策略正则化可能会将代理的探索空间限制在一个只有次优动作的小区域内，然后Q学习将被诱导收敛到一个次优策略。

​	策略正则化的不准确主要有两个原因：1.策略类别的表达能力不够；2.正则化方法不当。在大多数先前工作中，策略是由神经网络输出指定均值和对角协方差的高斯分布。然而，由于离线数据集通常是由多种策略混合收集的，这就使得不能很好地由对角高斯策略建模。为此，本文提出了一种使用扩散模型进行策略正则化的方法。具体来说，本文使用基于MLP的DDPM作为策略，构建了一个包含两个项的扩散损失目标：1.一个行为克隆项（**behavior cloning，用于正则**），鼓励扩散模型以与训练集相同的分布采样动作;2.一个策略改进项，试图采样高价值动作（根据学习到的Q值）。扩散模型是一个条件模型，以状态为条件，动作为输出。

​	**为什么使用扩散模型？**它有几个特性：首先，扩散模型非常具有表达能力，能够很好地捕捉**多峰分布**。其次，扩散模型损失构成了一种强大的分布匹配技术，因此它可以被视为一种强大的基于样本的策略正则化方法，无需额外的行为克隆。第三，扩散模型通过迭代细化进行生成，并且可以从最大化Q值函数的指导中，在每个逆向扩散步骤中添加。

# 前置知识

## 关于RL中的扩散模型

​	有人提出通过表达性强且稳定的扩散模型更好地模仿人类行为。Diffuser将扩散模型作为轨迹生成器应用。该工作将状态-动作对的完整轨迹形成一个扩散模型的单个样本，然后学习一个单独的回报模型来预测每个轨迹样本的累积奖励。然后，**回报模型的指导被注入到逆向采样阶段**。

# DQL

## 扩散策略

​	首先符号说明：使用上标 i∈{1,…,N}来表示扩散时间步，使用下标 t∈{1,…,T}来表示轨迹时间步。

​	通过条件扩散模型的逆向过程，将RL策略表示为：
$$
\pi_\theta(a\mid s)=p_\theta(a^{0:N}\mid s)=\mathcal{N}(a^N;0,I)\prod_{i=1}^Np_\theta(a^{i-1}\mid a^i,s)
$$
​	逆向过程的最后输出$a^0$就是用于RL评估的动作。首先采样一个服从于正态分布的$a^N$，则通过重参数化的技巧，每步扩散结果为（1）：
$$
a^{i-1}\mid a^{i}=\frac{a^{i}}{\sqrt{\alpha_{i}}}-\frac{\beta_{i}}{\sqrt{\alpha_{i}(1-\bar{\alpha}_{i})}}\epsilon_{\theta}(a^{i},s,i)+\sqrt{\beta_{i}}\epsilon
$$
​	去噪模型就用MLP表示，优化目标（2）与DDPM相同：
$$
\mathcal{L}_{d}(\theta)=\mathbb{E}_{i\sim\mathcal{U},\epsilon\sim\mathcal{N}(\mathbf{0},\boldsymbol{I}),(\boldsymbol{s},\boldsymbol{a})\sim\mathcal{D}}\left[||\epsilon-\epsilon_{\theta}(\sqrt{\bar{\alpha}_{i}}a+\sqrt{1-\bar{\alpha}_{i}}\epsilon,s,i)||^{2}\right]
$$
​	这个扩散损失即一个behavior cloning损失，用来学习behavior policy。**逆向扩散链的边缘分布提供了一个隐式的、表达性强的分布（因为不是直接给出概率密度，而是通过扩散过程逐渐生成的），能够捕捉离线数据集所表现出的复杂分布特性，如偏斜性和多峰性。且这种方法是基于采样的，我们不需要知道行为策略是什么。**

## Q学习

​	只有Ld损失的话，我们不可能取得比behavior policy更好的策略。因此我们需要用Q-value去指导扩散过程，来让策略去采样更高价值的动作。因此最终的策略学习目标函数（3）是：
$$
\pi=\arg\min\mathcal{L}(\theta)=\mathcal{L}_d(\theta)+\mathcal{L}_q(\theta)=\mathcal{L}_d(\theta)-\alpha\cdot\mathbb{E}_{\boldsymbol{s}\sim\mathcal{D},\boldsymbol{a}^0\sim\pi_\theta}\left[Q_\phi(s,\boldsymbol{a}^0)\right]
$$
​	这里的Q函数就用一种传统方式学习，且用到一种double Q-learning的trick。这里用到两个Q学习网络和两个Q目标网络，然后优化下面目标函数（4）来更新参数：
$$
\mathbb{E}_{(\boldsymbol{s}_t,\boldsymbol{a}_t,\boldsymbol{s}_{t+1})\sim\mathcal{D},\boldsymbol{a}_{t+1}^0\sim\pi_{\theta'}}\left[\left|\left|\left(r(s_t,a_t)+\gamma\min_{i=1,2}Q_{\phi'}(s_{t+1},a_{t+1}^0)\right)-Q_{\phi_i}(s_t,a_t)\right|\right|^2\right]
$$
​	算法图：

![image](https://cdn.jsdelivr.net/gh/Ameshiro77/BlogPicture/pic/image-20240925005156589.png)

