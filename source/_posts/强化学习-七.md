---
uuid: 1a6e4db0-3c2f-11ee-9a79-297de91ed3c2
title: '强化学习(七):时序差分算法'
author: Ameshiro
top: false
cover: 'https://s2.loli.net/2023/04/03/pvexKZFJ94oGbu8.jpg'
toc: true
mathjax: true
copyright_author: Ameshiro
hidden: false
tags:
  - 强化学习
categories:
  - 强化学习
abbrlink: bcf230a0
date: 2023-08-16 20:19:09
updated:
description:
img:
top_img:
password:
summary:
copyright:
copyright_author_href:
copyright_url:
copyright_info:
aplayer:
highlight_shrink:
aside:
---

​		这一章我们介绍Temporal-difference learning(TD-learning)，以及包含其中的Sarsa和Q-learning。我们先考虑下述表达式：
$$
w=\mathbb{E}[R+\gamma v(X)]
$$
​		这是对上一章的一个更复杂的例子。R和X都是随机变量，v是一个函数，$\gamma$是一个常数。那我们也可以定义出个g(w):
$$
g(w)=w-\mathbb{E}[R+\gamma v(X)]
$$
​		并且根据采样的r和x，得出对应的观测的$\tilde{g}$：
$$
\begin{aligned}
\tilde{g}\left(w,\eta\right)& =w-[r+\gamma v(x)]  \\
&=(w-\mathbb{E}[R+\gamma v(X)])+(\mathbb{E}[R+\gamma v(X)]-[r+\gamma v(x)]) \\
&\doteq g(w)+\eta.
\end{aligned}
$$
​		于是，我们就可以和RM算法联系起来，得到求解g(w)=0的迭代公式：
$$
w_{k+1}=w_{k}-\alpha_{k}\tilde{g}(w_{k},\eta_{k})=w_{k}-\alpha_{k}[w_{k}-(r_{k}+\gamma v(x_{k}))]
$$
​		这个表达式就和我们待会看TD算法的时候看到的表达式非常像了。这个R就对应的reward，$\gamma$就是衰减因子，v就是state value。

# TD算法介绍

​		先看如何用TD算法求解给定策略π后的state value，也就是policy evaluation。TD算法是一大类算法（包括sarsa，q-learning），也指用来估计state value的特定算法，也就是我们现在要介绍的。

​		TD算法基于数据（experience）来强化学习，这些数据是根据策略π得到的，它们可以写作：$(s_0,r_1,s_1,\ldots,s_t,r_{t+1},s_{t+1},\ldots)$或者$\{(s_t,r_{t+1},s_{t+1})\}_t$。那么我们期待已久的TD算法就长这样：
$$
\color{blue}
\begin{aligned}v_{t+1}(s_t)&=v_t(s_t)-\alpha_t(s_t)\Big[v_t(s_t)-[r_{t+1}+\gamma v_t(s_{t+1})]\Big],\quad&(1)\\v_{t+1}(s)&=v_t(s),\quad\forall s\neq s_t,\quad&(2)\end{aligned}
$$
​		我们注意，智能体在一个时刻只能访问到一个状态。这个算法就是，在t时刻，如果访问了状态空间的s状态，就更新这个状态的state value；而对于其他没有访问的状态，就保持不动。

​		大多数时候，第二个公式是忽略的。第一个公式的详细标注为：
$$
\underbrace{v_{t+1}(s_t)}_{\text{new estimate}}=\underbrace{v_t(s_t)}_{\text{current estimate}}-\alpha_t(s_t)[\overbrace{v_t(s_t)-[\underbrace{r_{t+1}+\gamma v_t(s_{t+1})}_{\text{TD target }\bar{v}_t}^{}]}^{\text{TD error }\delta_t}]
$$
​		其中 $\bar{v}_{t}\doteq r_{t+1}+\gamma v(s_{t+1})$ 叫做TD target，我们希望$v_t$朝着这个方向去修改。TD target和现在的value有个误差，叫做TD error。总而言之，新的estimate是由旧的estimate加上一个修正项得到的。下面对TD target 和TD error详细介绍。

​		第一，为什么$\bar{v}_{t}$叫做TD target。这是因为TD算法要让$v(s_t)$朝着$\bar{v}_{t}$去改进。为了看出这点，我们做以下变形，两边同时减去TD target：
$$
\begin{aligned}
&v_{t+1}(s_{t})=v_{t}(s_{t})-\alpha_{t}(s_{t})\big[v_{t}(s_{t})-\bar{v}_{t}\big] \\
\Longrightarrow & v_{t+1}(s_{t})-\bar{v}_{t}=v_{t}(s_{t})-\bar{v}_{t}-\alpha_{t}(s_{t})\big[v_{t}(s_{t})-\bar{v}_{t}\big]  \\
\Longrightarrow & v_{t+1}(s_{t}){-}\bar{v}_{t}=[1-\alpha_{t}(s_{t})][v_{t}(s_{t}){-}\bar{v}_{t}]  \\
\Longrightarrow & |v_{t+1}(s_{t})-\bar{v}_{t}|=|1-\alpha_{t}(s_{t})||v_{t}(s_{t})-\bar{v}_{t}| 
\end{aligned}
$$
​		由于$\alpha$是一个小的正数，那么显然$0<1-\alpha<1$，那么有：
$$
|v_{t+1}(s_t){-}\bar{v}_t|\leq|v_t(s_t){-}\bar{v}_t|
$$
​		这也意味着我们是朝着TD target前进的。

​		第二就是TD error：$\delta_{t}=v(s_{t})-[r_{t+1}+\gamma v(s_{t+1})]$ 。它表明了两个时间步的差，因此叫时序差分。它也反映了$v_t$和$v_\pi$之间的差。为什么这么说，因为当二者相等时，$\delta_{t}$就应该为0。原因如下：我们把公式的$v$改成$v_\pi$ ,故而有：
$$
\delta_{\pi,t}\doteq v_{\pi}(s_{t})-[r_{t+1}+\gamma v_{\pi}(s_{t+1})]
$$
​		我们对其求期望，得到：
$$
\mathbb{E}[\delta_{\pi,t}|S_{t}=s_{t}]=v_{\pi}(s_{t})-\mathbb{E}\big[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s_{t}\big]=0.
$$
​		因此在期望意义下，TD error能说明$v_t$和$v_\pi$之间的不一致。

​		TD有什么缺点呢？它只能估计给定一个策略下的state value，不能估计action value，也不能寻找最优策略，因为TD只是在policy estimation。但是在此基础上，能让他改的可以干这两件事。

​		从数学上讲TD算法是在干什么呢？它是在解一个给出策略后的贝尔曼方程，不同于我们最开始的基于模型的，TD是在不基于模型的条件下解贝尔曼方程。我们先给出一种贝尔曼方程的新表达式：
$$
v_{\pi}(s)=\mathbb{E}[R+\gamma G|S=s],\quad s\in\mathcal{S}
$$
​		我们令$S'$是下一个状态，那么显然可以写成：
$$
v_{\pi}(s)=\mathbb{E}[R+\gamma v_{\pi}(S^{\prime})|S=s],\quad s\in\mathcal{S}.
$$
​		这个式子也叫做Bellman expectation equation，TD算法就是在解这样一个equation。我们利用上节课所学的RM算法，定义：
$$
g(v(s))=v(s)-\mathbb{E}\big[R+\gamma v_\pi(S^{\prime})|s\big]=0
$$
​		

​		根据定义我们知道$v_\pi(s)$是这个方程的解。如何求解这个式子？我们有一系列对于R和S'的采样，因此就可以observation：
$$
\begin{aligned}
\tilde{g}(v(s))& =v(s)-\begin{bmatrix}r+\gamma v_\pi(s')\end{bmatrix}  \\
&\begin{aligned}=\underbrace{\left(v(s)-\mathbb{E}\big[R+\gamma v_{\pi}(S')|s\big]\right)}_{g(v(s))}+\underbrace{\left(\mathbb{E}\big[R+\gamma v_{\pi}(S')|s\big]-\big[r+\gamma v_{\pi}(s')\big]\right)}_{\eta}.\end{aligned}
\end{aligned}
$$
​		对应的RM算法就可以写成：
$$
\begin{aligned}
v_{k+1}(s)& \begin{aligned}=v_k(s)-\alpha_k\tilde{g}(v_k(s))\end{aligned}  \\
&=v_k(s)-\alpha_k\Big(v_k(s)-\big[{r_k+\gamma}{v_\pi(s_k^{\prime})}\big]\Big),\quad k=1,2,3,\ldots 
\end{aligned}
$$
​		这个和我们的TD算法是非常类似的，但是有两个问题，一个是他需要数据集合{(s,r,s')} for k=1,2,3,与TD基于episode的取样不一样；第二个是$v_\pi(s')$不知道。

​		为了解决第一个问题，我们把{(s,r,s')}换成$\{(s_t,r_{t+1},s_{t+1})\}$,为了解决第二个问题，我们把$v_\pi(s')$换成$v_k(s')$，这时的收敛性是可以保持的。具体证明就不阐述了。

​		TD和MC有什么区别?首先，前者是on-line的，即是一个episode是边采边更新的；后者是off-line的，即一个episode全采完后才能更新。其次，前者因为是on-line的，能够处理continuing tasks，后者只能处理episodic tasks。**这些很重要，经常看论文能看到这些词汇。**至于其他的，前者需要initial guesses，后者不需要；前者因为采样随机变量少，具有low estimation variance，后者具有high estimation variance。

# Sarsa

​		我们要policy improvement的话，需要知道action value，哪个value大就选哪个；因此我们引进Sarsa，来估计action value。我们将此和policy improvement结合起来，就能找到optimal policy。加入我们现在有如下数据：$\{(s_t,a_t,r_{t+1},s_{t+1},a_{t+1})\}_t$（看出来了嘛？这就是为什么叫sarsa，s a r s a），那么我们可以利用sarsa算法来更新：
$$
\begin{aligned}
q_{t+1}(s_{t},a_{t})& =q_t(s_t,a_t)-\alpha_t(s_t,a_t)\Big[q_t(s_t,a_t)-[r_{t+1}+\gamma q_t(s_{t+1},a_{t+1})]\Big],  \\
q_{t+1}(s,a)& =q_t(s,a),\quad\forall(s,a)\neq(s_t,a_t), 
\end{aligned}
$$
​		类似于TD，sarsa也是在解一个贝尔曼公式，这个公式是：
$$
q_\pi(s,a)=\mathbb{E}\left[R+\gamma q_\pi(S^{\prime},A^{\prime})|s,a\right],\quad\forall s,a.
$$
​		我们的最终目的是找到最优策略，所以我们还要把sarsa和policy improvement结合起来（实际上大多数时候听到的sarsa是包括了后者的）。sarsa伪代码如下：

<img src="https://s2.loli.net/2023/08/26/1NiX43Y9IEeSQrw.png" alt="image-20230819202850039" style="zoom:67%;" />

​		注意我们policy improvement采用的是epsilon-greedy。

# Expected Sarsa

​		这是对经典sarsa的改进，公式为：
$$
\begin{aligned}
q_{t+1}(s_{t},a_{t})& \begin{aligned}=q_t(s_t,a_t)-\alpha_t(s_t,a_t)\Big[q_t(s_t,a_t)-(r_{t+1}+\gamma\mathbb{E}[q_t(s_{t+1},A)])\Big],\end{aligned}  \\
q_{t+1}(s,a)& \begin{aligned}=q_t(s,a),\quad\forall(s,a)\neq(s_t,a_t),\end{aligned} 
\end{aligned}
$$
​		唯一的差别在于公式一的后面。改动的项具体为：
$$
\mathbb{E}[q_{t}(s_{t+1},A)])=\sum_{a}\pi_{t}(a|s_{t+1})q_{t}(s_{t+1},a)\doteq v_{t}(s_{t+1})
$$
​		就是把action value变成了state value。因为期望，它需要更多计算量，但是由于不需要对$a_{t+1}$采样，所以随机变量减少了，随机性也变小了。同样的，expected sarsa也在求解一个贝尔曼公式：
$$
q_{\pi}(s,a)=\mathbb{E}\Big[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s,A_{t}=a\Big]
$$

# n-step Sarsa

​		它包含了sarsa和蒙特卡洛两种方法。考虑我们的回报$G_t$

的不同写法：	
$$
\begin{aligned}
\mathsf{Sarsa}\longleftrightarrow G_{t}^{(1)}  & =R_{t+1}+\gamma q_{\pi}(S_{t+1},A_{t+1}),  \\
G_{t}^{(2)}& =R_{t+1}+\gamma R_{t+2}+\gamma^2q_\pi(S_{t+2},A_{t+2}),  \\
\text{:} \\
n\text{-step Sarsa}\longleftarrow G_{t}^{(n)}& =R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n}q_{\pi}(S_{t+n},A_{t+n}),  \\
\text{:} \\
\mathsf{MC}\longleftarrow G_t^{(\infty)}& =R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+\ldots  
\end{aligned}
$$
​			当n趋近于正无穷，就变成了MC。那些然，居于Sarsa和MC中间的就是n-step sarsa，它的公式显然就是：
$$
\begin{aligned}q_{t+1}(s_t,a_t)&=q_t(s_t,a_t)\\&-\alpha_t(s_t,a_t)\Big[q_t(s_t,a_t)-[r_{t+1}+\gamma r_{t+2}+\cdots+\gamma^nq_t(s_{t+n},a_{t+n})]\Big]\end{aligned}
$$

# Q-learning

​		直到今天Q-learning仍然在广泛使用，虽然用的是DQN。Q-learning从数学上讲，与sarsa区别是它主要是直接估计最优action value，就不需要PE和PI交替进行了。Q-learning算法公式为：
$$
\color{red}
\begin{aligned}
q_{t+1}(s_{t},a_{t})& \begin{aligned}=q_t(s_t,a_t)-\alpha_t(s_t,a_t)\left[q_t(s_t,a_t)-[r_{t+1}+\gamma\max_{a\in\mathcal{A}}q_t(s_{t+1},a)]\right]\end{aligned}  \\
q_{t+1}(s,a)& \begin{aligned}=q_t(s,a),\quad\forall(s,a)\neq(s_t,a_t),\end{aligned} 
\end{aligned}
$$
​		和Sarsa很像。区别在于TD target，Sarsa的TD target是$\begin{aligned}r_{t+1}+\gamma q_{t}(s_{t+1},a_{t+1})\end{aligned}$而Q-learning里是$r_{t+1}+\gamma\max_{a\in\mathcal{A}}q_{t}(s_{t+1},a)$。在数学上，Q-learning解决如下数学问题，他是在求解一个贝尔曼最优方程而非贝尔曼方程：
$$
\left.q(s,a)=\mathbb{E}\left[R_{t+1}+\gamma\max_{a}q(S_{t+1},a)\right|S_{t}=s,A_{t}=a\right],\quad\forall s,a.
$$
​		为了更详细介绍Q-learning，**先介绍两个重要概念：on-policy和off-policy。**我们定义，behavior policy是用来生成经验样本的，target policy是用来不断朝着最优策略更新的。如果二者相同，那么就叫on-policy；不同就是off-policy。额，通俗的讲，就是前者是自己做自己学，后者是可以拿别人的给自己学（并不是说不能自己做自己学）。我们的sarsa是前者，Q-learning是后者。

​		off-policy的好处就是可以把别人产生的experience拿来自己用。如何判断一个RL算法是on policy还是off policy？第一就是看解决什么问题（贝尔曼还是贝尔曼最优）；第二是看算法需要什么东西。sarsa和MC是off policy的，为什么Q-learning是on-policy?

​		因为它求解贝尔曼最优方程，它显式地不含任何策略；其次，它需要$\{(s_t,a_t,r_{t+1},s_{t+1},a_{t+1})\}_t$，并不需要$a_{t+1}$；而$r_{t+1},s_{t+1}$是依赖于p而不是π的，所以它的behavior policy是根据策略从$s_t$生成$a_t$，而target policy是选q大的action。

​		Q-learning可以on policy也可以off-policy，二者伪代码分别如下：

<img src="https://s2.loli.net/2023/08/19/FaLzDQ7wUWqnf2g.png" alt="image-20230819212008605" style="zoom:67%;" />

<img src="https://s2.loli.net/2023/08/19/C2jFOapIRuvigBd.png" alt="image-20230819212019924" style="zoom:67%;" />

伪代码里的$\pi_b$就是behavior policy。

# 统一总结

​		所有介绍过的算法被写成以下形式：
$$
q_{t+1}(s_t,a_t)=q_t(s_t,a_t)-\alpha_t(s_t,a_t)[q_t(s_t,a_t)-\bar{q}_t].
$$
​		并且：

<img src="https://s2.loli.net/2023/08/19/YsLgbUmZdHOVeWQ.png" alt="image-20230819213341687" style="zoom:67%;" />

其中，只有Q-learning在求解BOE。
