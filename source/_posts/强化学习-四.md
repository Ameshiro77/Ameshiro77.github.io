---
uuid: ab9f5460-2a9a-11ee-834a-c7c51fea7b3e
title: '强化学习(四):值迭代与策略迭代'
author: Ameshiro
top: false
cover: 'https://s2.loli.net/2023/07/25/ChnIkTPGgj1FWsN.jpg'
toc: true
mathjax: true
copyright_author: Ameshiro
hidden: false
tags:
  - 强化学习
categories:
  - 强化学习
abbrlink: b7629a84
date: 2023-07-25 11:23:48
updated:
description:
img:
top_img:
password:
summary:
copyright:
copyright_author_href:
copyright_url:
copyright_info:
aplayer:
highlight_shrink:
aside:
---

​		上一章的末尾我们提到了value iteration。这一章我们介绍value iteration和policy iteration——这二者都是截断策略迭代（Truncated policy iteration）的两个极端情况。  

# 值迭代

​		值迭代分两步：

·1.policy update（PU）。

这一步求解当给出$v_k$时的$\pi_{k+1}$，即$\pi_{k+1}=\arg\max_{\pi}(r_{\pi}+\gamma P_{\pi}v_{k})。$这里的$\pi_{k+1}$叫做greedy policy，因为它仅仅是在选择最大的q值。

·2.value update（VU）。

这一步求解$v_{k+1}$,即$v_{k+1}=r_{\pi_{k+1}}+\gamma P_{\pi_{k+1}}v_{k}$。 总的求解流程如下：
$$
v_k(s)\to q_k(s,a)\to\text{greedy policy }\pi_{k+1}(a|s)\to\text{new value\ }v_{k+1}=\max_aq_k(s,a)
$$


​		以上都是向量形式，为了给出具体算法，我们要将向量形式改写成元素形式（前者更适合理论分析）。伪代码如下。

<img src="https://s2.loli.net/2023/07/25/6RTmIJ91jAn4CKL.png" alt="image-20230725173848947" style="zoom:67%;" />

# 策略迭代

​		不同于值迭代，策略迭代是先给出个初始策略$\pi_0$.策略迭代也分两步：

·1.policy evaluation（PE）。 

这一步是计算贝尔曼方程的v。计算的方法是迭代方法，我们之前已经谈过了。

·2.policy improvement（PI）

根据step1算出的v，我们就可以得出每个状态的各个q值，进而更新greedy policy。

​		策略迭代的流程为：
$$
\pi_0\xrightarrow{PE}v_{\pi_0}\xrightarrow{PI}\pi_1\xrightarrow{PE}v_{\pi_1}\xrightarrow{PI}\pi_2\xrightarrow{PE}v_{\pi_2}\xrightarrow{PI}\dots 
$$
​		其伪代码为：

<img src="https://s2.loli.net/2023/07/25/bxCgPFH5TMA7ZLq.png" alt="image-20230725190035024" style="zoom:67%;" />

# 截断策略迭代

​		我们刚才讲的两个iteration是很相似的：
$$
\begin{aligned}\text{Policy iteration:}&\ \pi_0\xrightarrow{PE}&v_{\pi_0}\xrightarrow{PI}\pi_1\xrightarrow{PE}v_{\pi_1}\xrightarrow{PI}\pi_2\xrightarrow{PE}v_{\pi_2}\xrightarrow{PI}\dots\\
\text{Value iteration:}&\ &u_0\xrightarrow{PU}\pi_1^{\prime}\xrightarrow{VU}u_1\xrightarrow{PU}\pi_2^{\prime}\xrightarrow{VU}u_2\xrightarrow{PU}\dots\end{aligned}
$$
​		这实际是truncate policy iteration的两个极端情况。我们以下图为例：

<img src="https://s2.loli.net/2023/07/26/egdyr2nN7BK48Oz.png" alt="image-20230726005242945" style="zoom:67%;" />

​		考虑求解$v_{\pi_i}$。对于值迭代呢，我们就直接求出了v；但是对于策略迭代，我们需要无穷次的内部迭代才能求出v。而截断策略迭代就是指，我们能否在这无穷次的内部迭代中找到一个位置，来截断这一迭代过程。（不过实际上就算使用策略迭代也不可能无穷次迭代，一般会迭代到两次迭代误差小于一个阈值。）下面是伪代码，可以看到区别只是在于引入了截断阈值：

<img src="https://s2.loli.net/2023/07/26/Ancg2aNkHx8Lt3s.png" alt="image-20230726005728881" style="zoom:67%;" />

​		截断策略迭代会导致最后结果不收敛吗？实际是不会的，因为$v_{\pi_k}$总是随着迭代次数不断变大，证明就不给出了。三种算法的曲线如图：

<img src="https://s2.loli.net/2023/07/26/ANZYGaPDSMJ3xUH.png" alt="image-20230726005954880" style="zoom:67%;" />

​		对于截断策略迭代来说，截断的迭代次数越大，收敛就越快；但是太大了也不行，计算代价会很高。这是一个关于截断位置的曲线的比较：

<img src="https://s2.loli.net/2023/07/26/8D3f2oVuEtYNSrz.png" alt="image-20230726010424340" style="zoom:67%;" />
