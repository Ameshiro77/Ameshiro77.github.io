---
uuid: be943d70-3f25-11ee-9feb-fbeb74acd562
title: 强化学习(九):策略梯度方法
author: Ameshiro
top: false
cover: 'https://s2.loli.net/2023/04/03/pvexKZFJ94oGbu8.jpg'
toc: true
mathjax: true
copyright_author: Ameshiro
hidden: false
abbrlink: 7cc5cd62
date: 2023-08-18 14:49:43
updated:
description:
img:
top_img:
password:
summary:
tags:
  - 强化学习
categories:
  - 强化学习
copyright:
copyright_author_href:
copyright_url:
copyright_info:
aplayer:
highlight_shrink:
aside:
---

​		策略梯度方法和下一节的AC方法是现在越来越流行的方法。之前所有的方法都是value-based，现在开始的方法都是policy-based的。而基于策略的方法呢，是直接建立一个策略相关的目标函数（从value function 到 policy function），直接优化这个函数就会得到最优策略。而AC方法则是结合了基于策略和基于值的方法。

# 基本思想

​		目前为止，我们所有的策略都是表格表示的，如图所示：

<img src="https://s2.loli.net/2023/08/23/zChWPoQyvYDw3Fu.png" alt="image-20230823010059954" style="zoom:67%;" />

​		下面我们把表格改成函数，关于策略的写法也会改变：
$$
\pi(a|s,\theta) 或\pi_\theta(a|s)或\pi_\theta(a,s)
$$
​		这个$\theta$是一个向量，表示π这个函数里面的参数。现在用的最广泛的函数形式便是神经网络，输入s后，经过参数（策略函数是$\theta$,值函数是w），得到一组对应不同action的π。优点与之前讲值函数是一样的。

​		用表格表示的时候，我们说策略π是最优的，如果它能让每个状态价值最大；那如果用函数表示，该怎么定义最优策略？我们定义如果π最优，那么它就最大化了一个scalar metric，这个metric也在这里称为我们的目标函数。我们定义目标函数为$J(\theta)$。那么我们希望的$\theta^*$即为$argmax_\theta J(\theta)$。最简单的优化算法就是：
$$
\theta_{t+1}=\theta_t+\alpha\nabla_\theta J(\theta_t)
$$
​		别忘了我们是最大化而不是最小化。现在问题是，怎么取目标函数？怎么去求解对应梯度？这些还是有复杂的东西在里面的。

# 目标函数

## average value		

​		我们如何选择metric（即目标函数，我也不知道为什么叫法不一样）？在这里，我们会经常遇到两大类metric。

​		**第一大类叫average state value，或简称average value。**这就是对state value的一个加权平均，这个metric被定义为：
$$
\bar{v}_\pi=\sum_{s\in\mathcal{S}}d(s)v_\pi(s)
$$
​		d(s)是权重，和为1且都为正。这个$\bar{v}_\pi$呢就是个策略的函数，不同的策略对应的值也不同。当然，把d看成一个分布的话，也可以写成：
$$
\bar{v}_\pi=\mathbb{E}[v_\pi(S)]
$$
​		两个式子是一样的。当然了，也可以写成向量形式:
$$
\bar{v}_{\pi}=\sum_{s\in\mathcal{S}}d(s)v_{\pi}(s)=d^{T}v_{\pi}
$$
​		这样的形式有助于我们分析。那么我们怎么去选择分布d呢？第一种情况就是d是独立于策略π的，这样我们求梯度的时候就只需要求$v_\pi$的梯度。这种情况下，我们就把d和$\bar{v}_\pi$写成$d_0$和$\bar{v}_\pi^0$。最简单的情况就是均匀分布：$d_0$=1/|S|；或者，我们关心一些特殊的状态，比如$s_0$作为我们游戏开始的状态。极端情况是我们只关心这个状态，这样就变成了$d_0(s_0)=1,\quad d_0(s\neq s_0)=0$。

​		第二种情况就是d和π有关系。这种方式很常见：我们选定d为基于π的平稳分布（之前讲过），即$d_\pi(s)$。正如上节所说，只要知道$P_\pi$，就能知道这个平稳分布（当然也可能不知道P，不展开细说了..）。反正，访问多的状态权重应该多一些，反之则少。

​		**实际上，我们如果读论文的话，我们还会见到$\bar{v}_\pi$的另一种写法**：
$$
J(\theta)=\mathbb{E}\left[\sum_{t=0}^\infty\gamma^tR_{t+1}\right]
$$
​		证明的公式为：
$$
\begin{aligned}
\begin{aligned}J(\theta)=\mathbb{E}\left[\sum_{t=0}^\infty\gamma^tR_{t+1}\right]\end{aligned}& \begin{aligned}=\sum_{s\in\mathcal{S}}d(s)\mathbb{E}\left[\sum_{t=0}^\infty\gamma^tR_{t+1}|S_0=s\right]\end{aligned}  \\
&=\sum_{s\in\mathcal{S}}d(s)v_{\pi}(s) \\
&=\bar{v}_{\pi}
\end{aligned}
$$

## Average reward

​		第二种目标哈数就是average one-step reward，简称averagereward。它的函数被定义为：
$$
\bar{r}_\pi\doteq\sum_{s\in\mathcal{S}}d_\pi(s)r_\pi(s)=\mathbb{E}[r_\pi(S)]
$$
​		这里，
$$
r_\pi(s)\doteq\sum_{a\in\mathcal{A}}\pi(a|s)r(s,a)
$$
​		这里的权重d是依赖于策略π的，是stationary distribution。这里的r(s,a)也是一个均值含义上的immediate reward，它可以写作：
$$
r(s,a)=\mathbb{E}[R|s,a]=\sum_rrp(r|s,a)
$$
​		现在我们给出第二种等效的形式，在我们看论文看书的时候会经常遇到。假设我们现在有个策略，依据这个策略形成了一个trajectory，得到了很多reward:$(R_{t+1},R_{t+2},\ldots)$;我们把这些reward加起来求个期望再求个平均，得到如下式子：
$$
\begin{aligned}
&\lim_{n\to\infty}\frac1n\mathbb{E}\Big[R_{t+1}+R_{t+2}+\cdots+R_{t+n}|S_t=s_0\Big] \\
&=\lim_{n\to\infty}\frac1n\mathbb{E}\left[\sum_{k=1}^nR_{t+k}|S_t=s_0\right]
\end{aligned}
$$
​		它代表了跑无穷多步时reward的平均是什么、跑了无穷多步后，我们的$s_0$就没用了，所以这个式子也可以写为：
$$
\lim_{n\to\infty}\frac1n\mathbb{E}\left[\sum_{k=1}^nR_{t+k}|S_t=s_0\right]=\lim_{n\to\infty}\frac1n\mathbb{E}\left[\sum_{k=1}^nR_{t+k}\right]
$$
​		**这玩意就是我们刚才说过的$\bar{r}_\pi$。**主要我们这里是immediate reward，对于discounted case和undiscounted case都是成立的。

## 分析

​		直观上，由于$\bar{v}_\pi$用了return而$\bar{r}_\pi$用了immediate reward，后者似乎要更short-sighted。然而实际上，两个metric是等价的。实际上我们可以证明：
$$
\bar{r}_\pi=(1-\gamma)\bar{v}_\pi
$$
​		证明略。这就说明，一方达到最优，那么另一方也就达到了最优。

# 梯度计算

​		计算梯度是整个策略梯度方法中最复杂的一部分。一是因为我们有很多不同的metric，二是因为我们要区分discounted和undiscounted的情况。但无论如何，**这些梯度的结果都可以总结为这样的公式**：
$$
\nabla_\theta J(\theta)=\sum_{s\in\mathcal{S}}\eta(s)\sum_{a\in\mathcal{A}}\nabla_\theta\pi(a|s,\theta)q_\pi(s,a)
$$
​		这里的等号可以是严格等于或者约等于或者成比例等于。$\eta$在不同情况下对不同目标函数呈现出不同的分布。绝大情况认识这个式子就够了，除非是要科研创新。我们给出一些具体的例子：
$$
\nabla_\theta\bar{r}_\pi\simeq\sum_sd_\pi(s)\sum_a\nabla_\theta\pi(a|s,\theta)q_\pi(s,a)
$$
​		我们又知道$\bar{v}_\pi$和$\bar{r}_\pi$是有关系的，因此我们有：
$$
\nabla_{\theta}\bar{v}_{\pi}=\frac{1}{1-\gamma}\nabla_{\theta}\bar{r}_{\pi}
$$
​		详细细节就不介绍了。现在我们对这些梯度作进一步分析。最重要的性质就是，梯度可以把所有的求和符合去掉，写成如下期望的形式：
$$
\begin{aligned}\color{blue}{\nabla_\theta J(\theta)}&=\sum_{s\in\mathcal{S}}\eta(s)\sum_{a\in\mathcal{A}}\nabla_\theta\pi(a|s,\theta)q_\pi(s,a)\\&=\color{blue}{\mathbb{E}[\nabla_\theta\ln\pi(A|S,\theta)q_\pi(S,A)]}\end{aligned}
$$
​		这里的(S,A)是随机变量，S满足$\eta$分布。先不说推导，为什么我们需要这样的式子？类似SGD，**因为我们可以用采样的方式近似这个期望**。**这样，我们就可以用SGD来优化**：
$$
\nabla_\theta J\approx\nabla_\theta\ln\pi(a|s,\theta)q_\pi(s,a)
$$
​		下面我们来看一下怎么得到这个公式。考虑我们出现的lnπ，这样就方便我们求导了：
$$
\nabla_\theta\ln\pi(a|s,\theta)=\frac{\nabla_\theta\pi(a|s,\theta)}{\pi(a|s,\theta)}
$$
​		这样我们变个型，就有了：
$$
\nabla_\theta\pi(a|s,\theta)=\pi(a|s,\theta)\nabla_\theta\ln\pi(a|s,\theta)
$$
​		我们把这个式子代入梯度的表达式里，就可以得到期望的形式了：
$$
\begin{aligned}
\nabla_{\theta}J& =\sum_sd(s)\sum_a\color{blue}{\nabla_\theta\pi(a|s,\theta)q_\pi(s,a)}  \\
&=\sum_sd(s)\sum_a\pi(a|s,\theta)\nabla_\theta\ln\pi(a|s,\theta)q_\pi(s,a) \\
&=\mathbb{E}_{\color{blue}{S\sim d}}\left[\sum_a\pi(a|S,\theta)\nabla_\theta\ln\pi(a|S,\theta)q_\pi(S,a)\right] \\
&=\mathbb{E}_{S\sim d,\color{blue}{A}\sim\pi}\left[\nabla_\theta\ln\pi(A|S,\theta)q_\pi(S,A)\right] \\
&\doteq\mathbb{E}\big[\nabla_\theta\ln\pi(A|S,\theta)q_\pi(S,A)\big]
\end{aligned}
$$
这里的$\mathbb{E}_{S\sim d}$就是说S满足d的分布。现在我们还要做一些补充说明。首先，不同于之前，lnπ必须要让π大于0；**因此我们引入softmax function**，把这些π归一化到(0,1)区间。学过ML的读者对此应该不会陌生。具体来讲，就是采用以下公式：
$$
\pi(a|s,\theta)=\frac{e^{h(s,a,\theta)}}{\sum_{a^{\prime}\in\mathcal{A}}e^{h(s,a^{\prime},\theta)}}
$$
​		这里的h是另一个函数，类似于我们之前介绍value function approximation里的feature funtion。当然了现在基本都用神经网络了，输出层接个softmax就行了。由于我们的每个π都是大于0的，所以我们的策略是stochastic的，有探索性。但是如果action有无穷多个呢？那这种方法就不行了；但是接下来讲的deterministic policy gradient是可以的。

# 梯度上升与REINFORCE

​		梯度上升的基本思路是：
$$
\begin{aligned}
\theta_{t+1}& =\theta_t+\alpha\nabla_\theta J(\theta)  \\
&=\theta_t+\alpha\mathbb{E}\Big[\nabla_\theta\ln\pi(A|S,\theta_t)q_\pi(S,A)\Big]
\end{aligned}
$$
​		实际上这是不能用的，因为有个期望，我们因为不知道环境模型，所以分布不知道，算不了期望，因此我们用随机梯度代替，就得到了：
$$
\theta_{t+1}=\theta_t+\alpha\nabla_\theta\ln\pi(a_t|s_t,\theta_t)q_\pi(s_t,a_t)
$$
​		但是这个式子也用不了，因为有个真实的action value:$q_\pi$，我们不知道它。所以我们用某种方法近似它，用$q_t$代替$q_\pi$。第一种近似方法就是蒙特卡洛法，从(s,a)出发得到一个return，来近似代替。这种结合出来的方法就叫做REINFORCE。除此之外我们还可以用TD方法等等，这就引出了actor-critic，将在下一节介绍。

​		通常而言策略梯度是on-policy的，但是也有off-policy的，但是需要引入额外技巧，在下一节说明。下面我们再重新组织一下这个算法。由于：
$$
\nabla_\theta\ln\pi(a_t|s_t,\theta_t)=\frac{\nabla_\theta\pi(a_t|s_t,\theta_t)}{\pi(a_t|s_t,\theta_t)}
$$
​		因此算法可以重写成：
$$
\begin{aligned}
\theta_{t+1}& \begin{aligned}=\theta_t+\alpha\nabla_\theta\ln\pi(a_t|s_t,\theta_t)q_t(s_t,a_t)\end{aligned}  \\
&=\theta_t+\alpha\underbrace{\left(\frac{q_t(s_t,a_t)}{\pi(a_t|s_t,\theta_t)}\right)}_{\beta_t}\nabla_\theta\pi(a_t|s_t,\theta_t)
\end{aligned}
$$
​		这样我们就得到了算法的一种重要表达形式：
$$
\theta_{t+1}=\theta_{t}+\alpha\beta_{t}\nabla_{\theta}\pi(a_{t}|s_{t},\theta_{t})
$$
​		可以看出这是一个优化$\pi(a_{t}|s_{t},\theta_{t})$的算法，步长是$\alpha\beta_{t}$。从微分的角度，我们有：
$$
\pi(a_t|s_t,{\theta_{t+1}})\approx\pi(a_t|s_t,{\theta_t})+(\nabla_\theta\pi(a_t|s_t,{\theta_t}))^T({\theta_{t+1}-\theta_t})
$$
​		这是微分意义上的。如果$\theta_{t+1}-\theta_t$差太多，这种近似就不精确了。 当我们把原先梯度上升公式的$\theta_{t+1}-\theta_t$代入，就得到了：
$$
\begin{aligned}
\pi(a_t|s_t,\theta_{t+1})
&\begin{aligned}=\pi(a_t|s_t,\theta_t)+\alpha\beta_t(\nabla_\theta\pi(a_t|s_t,\theta_t))^T(\nabla_\theta\pi(a_t|s_t,\theta_t))\end{aligned} \\
&=\pi(a_t|s_t,{\theta_t})+\alpha\beta_t\|\nabla_\theta\pi(a_t|s_t,\theta_t)\|^2
\end{aligned}
$$
​		向量的模长肯定是大于等于0的，**所以我们可以知道：当$\beta_t>0$时，选择$(s_t,a_t)$的概率会增大，因为$\pi(a_{t}|s_{t},\theta_{t+1})>\pi(a_{t}|s_{t},\theta_{t})$，反之就是变小**。

​		$\beta_t$能够很好的平衡exploration和exploitation。为什么这么说？从$\beta_t$的定义可以看出，它与分子$q_t$成正比，当q比较大β也会比较大，这就意味着π值会被增大；这就意味着如果action value大，就会有更大的概率去选它，这就是exploitation。可是，β又与分母的π成反比，当π值比较小，β就会比较大，就会让π值变大，这说明如果我选择$a_t$的概率比较小，那下个时刻我会更大概率去选它，这是exploration。

​		现在来看具体算法。记住我们现在的公式变成了：
$$
\theta_{t+1}=\theta_{t}+\alpha\nabla_{\theta}\ln\pi(a_{t}|s_{t},\theta_{t})q_{t}(s_{t},a_{t})
$$
​		这里用的是$q_t$。如果我们用MC方法去估计它，那么这个算法的具体名字叫做REINFORCE，它是最早的最简单的策略梯度算法。REINFORCE的伪代码为：

<img src="https://s2.loli.net/2023/08/23/cWDawO2q8LNSoKB.png" alt="image-20230823141952332" style="zoom:67%;" />

​		注意我们的MC是off line的，我们必须把所有episode全采集完后开始跑，不能边更新$\theta_t$边采集。之后介绍基于TD的方法后，就可以变成on line的了。
