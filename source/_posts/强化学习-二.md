---
uuid: dc04a530-2264-11ee-87bc-5f7b5615fa8d
title: 强化学习(二):贝尔曼方程
author: Ameshiro
top: false
cover: 'https://s2.loli.net/2023/07/15/R5vd7pmhLZk9S6H.jpg'
toc: true
mathjax: true
copyright_author: Ameshiro
hidden: false
description: 强化学习笔记
tags:
  - 强化学习
  - 贝尔曼方程
categories:
  - 强化学习
abbrlink: b30ab213
date: 2023-07-15 00:38:27
updated:
img:
top_img:
password:
summary:
copyright:
copyright_author_href:
copyright_url:
copyright_info:
aplayer:
highlight_shrink:
aside:
---

​		Bellman Equation（贝尔曼方程）也称动态规划方程，是强化学习的基础和核心，是用来简化强化学习或者MDP问题的。**需要知道，贝尔曼方程是用来评价策略好坏的，所以我们可以在后面看到这个方程和策略$\pi$的关系紧密。**

# 引入

​		我们先前说过，评价policy好坏的重要标准是return（添加了discount rate后）。现在我们有如下三种策略：

<img src="https://s2.loli.net/2023/07/15/MCQdFjlBhg2Jw7A.png" alt="image-20230715010224007" style="zoom:67%;" />

​		我们依次计算return来评价它们好坏，假设衰减因子是$\gamma$.

policy 1:
$$
\begin{aligned}
\operatorname{return}_{1} & =0+\gamma 1+\gamma^{2} 1+\ldots \\
& =\gamma\left(1+\gamma+\gamma^{2}+\ldots\right) \\
& =\frac{\gamma}{1-\gamma}
\end{aligned}
$$
policy 2:
$$
\begin{aligned}
\operatorname{return}_{2} & =-1+\gamma 1+\gamma^{2} 1+\ldots \\
& =-1+\gamma\left(1+\gamma+\gamma^{2}+\ldots\right) \\
& =-1+\frac{\gamma}{1-\gamma}
\end{aligned}
$$
policy 3:
$$
\begin{aligned}
\operatorname{return}_{3} & =0.5(-1+\frac{\gamma}{1-\gamma})+0.5(\frac{\gamma}{1-\gamma})\\
&=-0.5+\frac{\gamma}{1-\gamma}
\end{aligned}
$$
​		**这里的return严格来说已经不是我们基本概念的return了**，因为基本概念的return是针对于一个trajectory而言的。我们这里算的其实是一个期望（expectation），也就是我们后面要说的state value。

​		计算出三个return后，显然1>3>2。因此第一个策略是最好的。那么如何计算return呢？刚才我们计算return是用的最本质的定义，现在我们换个例子如下：

<img src="https://s2.loli.net/2023/07/15/sETBKknzuIlFi58.png" alt="image-20230715011147584" style="zoom: 67%;" />

​		首先我们定义：$v_i$是当从状态$s_i$开始时，获得的return值。因此我们有如下等式：
$$
\begin{aligned}
v_1=r_1+\gamma r_2+\gamma^2r_3+…\\
v_2=r_2+\gamma r_3+\gamma^2r_4+…\\
v_3=r_3+\gamma r_4+\gamma^2r_1+…\\
v_4=r_4+\gamma r_1+\gamma^2r_2+…\\
\end{aligned}
$$
​		我们观察$v_1$. 我们提取出$\gamma$,就可以得到：
$$
v_1=r_1+\gamma (r_2+\gamma r_3+…)=r_1+\gamma v_2
$$
​		是不是有点动态规划的感觉？同理我们可以得到：
$$
\begin{aligned}
v_2=r_2+\gamma v_3 \\
v_3=r_3+\gamma v_4 \\
v_4=r_4+\gamma v_1 \\
\end{aligned}
$$
​		这组式子告诉我们，不同状态出发得到的return依赖于别的状态出发得到的return，这样的想法在RL中就被成为Bootstrapping，就是从自己出发解决自己。直观上这是不可能的，我们不妨写成更数学一点的矩阵形式：
$$
\begin{equation}
\left[\begin{array}{l}
v_1 \\
v_2 \\
v_3 \\
v_4
\end{array}\right]=\left[\begin{array}{l}
r_1 \\
r_2 \\
r_3 \\
r_4
\end{array}\right]+\left[\begin{array}{l}
\gamma v_2 \\
\gamma v_3 \\
\gamma v_4 \\
\gamma v_1
\end{array}\right]=\left[\begin{array}{l}
r_1 \\
r_2 \\
r_3 \\
r_4
\end{array}\right]+\gamma\left[\begin{array}{llll}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0
\end{array}\right]\left[\begin{array}{l}
v_1 \\
v_2 \\
v_3 \\
v_4
\end{array}\right]
\end{equation}
$$
​		可以写成如下：
$$
\mathbf{v}=\mathbf{r}+\gamma \mathbf{P} \mathbf{v}
$$
​		这个$\bf{r}$我们是知道的，$\bf{P}$我们也是知道的，它其实是对应state transition的一些东西。这个就是一个简单的贝尔曼公式，但它真的只是一个简单的，针对于deterministic case的贝尔曼公式，后面我们会介绍更正式的贝尔曼公式。

​		这个公式告诉我们，我们的状态的value依赖于别的状态的value（后面会定义），且矩阵形式可以帮助我们求解。这个例子的解就是
$$
\bf{v}=(I-\gamma\bf{P})^{-1}r
$$

# State value：状态价值

​		为介绍state value，我们先考虑一个单步过程：
$$
\begin{equation}
S_t \stackrel{A_t}{\longrightarrow} R_{t+1}, S_{t+1}
\end{equation}
$$
​		它包含以下元素：

·  $t,t+1$ : 当前时刻和下一时刻

· $S_t$ : 在t时刻所处状态

· $A_t$ : 在 $S_t$ 状态采取的行为

· $R_{t+1}$ : 采取$A_t$后获得的reward ， 有的地方可能写成$R_t$

· $S_{t+1}$ : 采取$A_t$后迁移到的状态

​		这里的$S_t,A_t,R_{t+1}$都是随机值。这些都是由一些概率分布决定的：

<img src="https://s2.loli.net/2023/07/15/Hs8M9fx1kdCIwY3.png" alt="image-20230715015548557" style="zoom:67%;" />

​		现在考虑一个多步的trajectory：
$$
\begin{equation}
S_t \stackrel{A_t}{\longrightarrow} R_{t+1}, S_{t+1} \stackrel{A_{t+1}}{\longrightarrow} R_{t+2}, S_{t+2} \stackrel{A_{t+2}}{\longrightarrow} R_{t+3}, \ldots
\end{equation}
$$
​		我们计算出discounted return为：
$$
\begin{equation}
G_t=R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\ldots
\end{equation}
$$
​		其中$\gamma$是衰减系数。$G_t$是随机变量，是因为$R_i$都是随机变量。现在我们知道了，$G_t$是对一个trajectory的discounted return，**那么state value就是$G_t$的期望值（state value全称叫state value function，即*状态价值函数*）**。我们用如下公式定义state value：
$$
\begin{equation}
v_\pi(s)=\mathbb{E}\left[G_t \mid S_t=s\right]
\end{equation}
$$
​		这是一个关于$s$的函数，并且基于策略$\pi$。对于不同的policy，state value可能会不一样。state value越大，说明处在这个状态可能会带来更大的收益。

​		**return和state value有什么区别？**return 是对单个trajectory求值，而state value是多个trajectory算出来结果的平均。当然，如果只有一个trajectory，那么二者相同。

# Derivation：贝尔曼公式的推导

​		首先根据前面对于$G_t$的定义，我们可以得到如下公式：
$$
\begin{equation}
\begin{aligned}
G_t & =R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\ldots, \\
& =R_{t+1}+\gamma\left(R_{t+2}+\gamma R_{t+3}+\ldots\right), \\
& =R_{t+1}+\gamma G_{t+1}
\end{aligned}
\end{equation}
$$
​		由此，根据期望的线性关系，我们把状态价值函数可以写成：
$$
\begin{equation}
\begin{aligned}
v_\pi(s) & =\mathbb{E}\left[G_t \mid S_t=s\right] \\
& =\mathbb{E}\left[R_{t+1}+\gamma G_{t+1} \mid S_t=s\right] \\
& =\mathbb{E}\left[R_{t+1} \mid S_t=s\right]+\gamma \mathbb{E}\left[G_{t+1} \mid S_t=s\right]
\end{aligned}
\end{equation}
$$
​		于是我们的任务就变成了分别求出两个子期望。下面给出推导.首先我们先画个示意图：

<img src="https://s2.loli.net/2023/07/15/ylXMxWfLzR7tOj4.jpg" alt="img" style="zoom: 15%;" />

①：$\mathbb{E}\left[R_{t+1} \mid S_t=s\right]$ :

​		这个相对来说简单一点，我们要求当前时刻的奖励，就是对采取不同行动时获得的奖励进行一个平均（也就是期望，离散情景下期望公式就是$\sum{XP(X)}$）。而我们采取何种行动取决于我们的策略，因此先对策略求平均：
$$
\begin{equation}
\begin{aligned}
\mathbb{E}\left[R_{t+1} \mid S_t=s\right] & =\sum_a \pi(a \mid s) \mathbb{E}\left[R_{t+1} \mid S_t=s, A_t=a\right] \\
\end{aligned}
\end{equation}
$$
​		这个式子后面的 $\mathbb{E}\left[R_{t+1} \mid S_t=s, A_t=a\right]$ 是我们具体的采取行动$a_i$时，获得的奖励，它是对reward probability，也就是 $p(r|s,a)$ 的一个期望计算：
$$
\mathbb{E}\left[R_{t+1} \mid S_t=s, A_t=a\right] =\sum_r p(r|s, a) r
$$
​		于是我们得到最终的结果为：
$$
\mathbb{E}[R_{t+1} \mid S_t]=\sum_a \pi(a \mid s) \sum_r p(r \mid s, a) r
$$
②.$\mathbb{E}[G_{t+1} \mid S_t=s]$

​		这个公式是在计算未来获得的return的期望。在我们的示意图中，他就表示为在$不同s'_i$ 时获得的各期望的一个加权平均。显然，直觉上来看，这类似于动态规划，最后的结果会包含$不同s'_i$ 的state value：$v_{\pi}$.

​		因此先有如下推导：
$$
\begin{equation}
\begin{aligned}
\mathbb{E}\left[G_{t+1} \mid S_t=s, S_{t+1}=s^{\prime}\right] p\left(s^{\prime} \mid s\right) 
& =\sum_{s^{\prime}} \mathbb{E}\left[G_{t+1} \mid S_{t+1}=s^{\prime}\right] p\left(s^{\prime} \mid s\right) \\
& =\sum_{s^{\prime}} v_\pi\left(s^{\prime}\right) p\left(s^{\prime} \mid s\right)
\end{aligned}
\end{equation}
$$
​		这里注意$S_t$变成了$S_{t+1}$，由于马尔科夫性质，我们要计算的东西跟$S_t$已经无关了。具体来说，这里其实省略了一步：
$$
\mathbb{E}\left[G_{t+1} \mid S_t=s, S_{t+1}=s^{\prime}\right]=\mathbb{E}\left[G_{t+1} \mid S_{t+1}=s^{\prime}\right]
$$
​		**这里尤其需要注意的是：这里乘的条件概率是$p(s'|s)$而非公式①里用的$\pi(a|s)$. 这里一定要区分清楚：在公式①中，我们算的是当前状态出发获得的reward，而reward是只依据当前状态和采取的行动的，是跟策略有关而跟之后到达什么状态无关的（因为我们还有state transition probability，并不是采取了一个行动就一定到达一个固定的状态）；而在公式②中，我们要计算从当前状态转移到另一个状态的条件概率，它不仅依赖于策略，还依赖于state transition probability！因此，这里写作$p(s'|s)$；我们可能采取多个行动使得从$s$达到$s'$,策略使我们决定采取什么行动，state transition probability决定我们到哪，那么它的值就是：**
$$
p(s'|s)= \sum_a p(s'|s,a)\pi(a|s)
$$
​		由此以来，最终的公式②推导便是：
$$
\begin{equation}
\begin{aligned}
\mathbb{E}\left[G_{t+1} \mid S_t=s\right] =\sum_{s^{\prime}} v_\pi\left(s^{\prime}\right) \sum_a p\left(s^{\prime} \mid s, a\right) \pi(a \mid s)
\end{aligned}
\end{equation}
$$
​		现在我们得到了两部分的公式，还记得我们最初的公式吗？合并起来就是：
$$
\begin{aligned}
\color{blue}{v_\pi(s)}& =\mathbb{E}[R_{t+1}|S_{t}=s]+\gamma\mathbb{E}[G_{t+1}|S_{t}=s],  \\
\\
&=\underbrace{\sum_a\pi(a|s)\sum_rp(r|s,a)r}_{\text{mean of immediate rewards}}+\underbrace{\gamma\sum_a\pi(a|s)\sum_{s^{\prime}}p(s^{\prime}|s,a)v_\pi(s^{\prime}),}_{\text{mean of future rewards}} \\
&={\sum_a\pi(a|s)\left[\sum_rp(r|s,a)r+\gamma\sum_{s^{\prime}}p(s^{\prime}|s,a)v_\pi(s^{\prime})\right]},\quad\forall s\in\mathcal{S}.
\end{aligned}
$$
​		注意这里合并的时候对②的求和进行了一个变形：
$$
\begin{aligned}
\sum_{s^{\prime}}v_\pi\left(s^{\prime}\right)\sum_{a^{\prime}}p\left(s^{\prime}\left|s,a\right)\pi\left(a\left|s\right)\right.\right.  & =\sum_{s^{\prime}}v_\pi\left(s^{\prime}\right)\sum_ag\left(s^{\prime},a\right)  \\
&=\sum_{s^{\prime}}\sum_av_\pi(s^{\prime})g(s^{\prime},a) \\
&=\sum_a\sum_{s^{\prime}}v_\pi\left(s^{\prime}\right)p\left(s^{\prime}|s,a\right)\pi\left(a|s\right) \\
&=\sum_a\pi\left(a|s\right)\sum_{s^{\prime}}v_\pi\left(s^{\prime}\right)p(s^{\prime}|s,a)
\end{aligned}
$$
​		这里把第二个$\sum$后面的看成g(s',a),然后利用了两个求和的性质：

1.与求和量无关的数可以提取出去（自然也可以放到求和里面）

2.有限次的无关的多重求和可以对换次序（$\sum_{i=1}^n\sum_{j=1}^i这种就是有关的$）；无限次的求和要保证级数一致收敛。

---

​		至此，我们得到了贝尔曼方程的一般形式：
$$
v_\pi(s)={\sum_a\pi(a|s)\left[\sum_rp(r|s,a)r+\gamma\sum_{s^{\prime}}p(s^{\prime}|s,a)v_\pi(s^{\prime})\right]},\quad\forall s\in\mathcal{S}
$$
​		之后我们会提及它的向量形式写法。求解这个方程叫做**policy evaluation**。$p(r|s,a)$和$p(s'|s,a)$代表动态模型，如果未知的话就不能用贝尔曼方程了。下面举个简单的例子来求解。

# 简单的求解举例

​		我们的例图如下：

<img src="https://s2.loli.net/2023/07/19/iUAF2yaI8VwRtgL.png" alt="image-20230719121033032" style="zoom:67%;" />

​		图中有四个状态，我们要列出四个状态的方程。这个例子中，状态1的reward和state transition都是deterministic的。我们列出四个方程：
$$
\begin{aligned}
&v_{\pi}(s_{1}) =0.5[0+\gamma v_{\pi}(s_{3})]+0.5[-1+\gamma v_{\pi}(s_{2})],  \\
&v_{\pi}(s_{2}) =1+\gamma v_{\pi}(s_{4}),  \\
&v_{\pi}(s_{3}) =1+\gamma v_{\pi}(s_{4}),  \\
&v_{\pi}(s_{4}) =1+\gamma v_{\pi}(s_{4}). 
\end{aligned}
$$
​		可以先求出$v_\pi(s4)$，然后轻松求出别的值。给定$\gamma$为0.9，可以算出s1~s4的state value为10,10,10,8.5。

# Matrix-vector form：贝尔曼方程的向量形式与求解

​		在贝尔曼方程里，我们令：
$$
r_\pi(s)\triangleq\sum_a\pi(a|s)\sum_rp(r|s,a)r,\quad p_\pi(s'|s)\triangleq\sum_a\pi(a|s)p(s'|s,a)
$$
​		代入贝尔曼方程，便得到$s_i$的贝尔曼方程：
$$
v_\pi(s_i)=r_\pi(s_i)+\gamma\sum_{s_j}p_\pi(s_j|s_i)v_\pi(s_j)
$$
​		我们把所有状态的贝尔曼方程放在一起，就可以得到向量形式：
$$
\bf{v_{\pi}}=r_{\pi}+\gamma P_{\pi}v_{\pi}
$$
​		把它写开，就张这个样子(4个状态为例)：
$$
\underbrace{\left[\begin{array}{c}v_\pi(s_1)\\v_\pi(s_2)\\v_\pi(s_3)\\v_\pi(s_4)\end{array}\right]}_{v_\pi}=\underbrace{\left[\begin{array}{c}r_\pi(s_1)\\r_\pi(s_2)\\r_\pi(s_3)\\r_\pi(s_4)\end{array}\right]}_{r_\pi}+\gamma\underbrace{\left[\begin{array}{cccc}p_\pi(s_1|s_1)&p_\pi(s_2|s_1)&p_\pi(s_3|s_1)&p_\pi(s_4|s_1)\\p_\pi(s_1|s_2)&p_\pi(s_2|s_2)&p_\pi(s_3|s_2)&p_\pi(s_4|s_2)\\p_\pi(s_1|s_4)&p_\pi(s_2|s_3)&p_\pi(s_3|s_4)&p_\pi(s_4|s_3)\\p_\pi(s_1|s_4)&p_\pi(s_2|s_4)&p_\pi(s_3|s_4)&p_\pi(s_4|s_4)\end{array}\right]}_{P_\pi}\underbrace{\left[\begin{array}{c}v_\pi(s_1)\\v_\pi(s_2)\\v_\pi(s_3)\\v_\pi(s_4)\end{array}\right]}_{v_\pi}
$$
​		其中$\bf{P_\pi}$叫做**state transition matrix. ** $\bf{r_\pi}$就代表了在当前状态采取各行动的奖励期望。我们还是举个简单的例子说明一下，以刚才说过的图例为例：

<img src="https://s2.loli.net/2023/07/19/iUAF2yaI8VwRtgL.png" alt="image-20230719121033032" style="zoom:67%;" />

​		它的贝尔曼方程的向量形式为：
$$
\left[\begin{array}{c}v_{\pi}(s_{1})\\v_{\pi}(s_{2})\\v_{\pi}(s_{3})\\v_{\pi}(s_{4})\end{array}\right]=\left[\begin{array}{c}0.5(0)+0.5(-1)\\1\\1\\1\end{array}\right]+\gamma\left[\begin{array}{cccc}0&0.5&0.5&0\\0&0&0&1\\0&0&0&1\\0&0&0&1\end{array}\right]\left[\begin{array}{c}v_{\pi}(s_{1})\\v_{\pi}(s_{2})\\v_{\pi}(s_{3})\\v_{\pi}(s_{4})\end{array}\right]
$$
​		说完向量形式，该说它的求解了。直观上看，显然贝尔曼方程有闭式解，且解为：
$$
\bf{v_{\pi}}=(I-\gamma P_{\pi})^{-1}r_{\pi}
$$
​		但是在实际中，求矩阵逆是很耗时的，而且通常是在状态数巨大的情况下。因此，我们需要用**迭代**的思想简化运算。这里就直接给结论了，具体的牵扯到数值计算误差分析的东西，后面就贴一下就行了(就是在证明误差最后趋近于0)。迭代公式为：
$$
\bf{v_{k+1}=r_\pi+\gamma P_\pi v_k}
$$
​		可以证明$k→∞，\bf{v_k→v_\pi}$.证明过程为：

<img src="https://s2.loli.net/2023/07/19/eK7N96a1E28sg5B.png" alt="image-20230719124112360" style="zoom: 50%;" />

# Action value：动作价值

​		state value指的是从一个状态出发得到的return期望，而action value是指从某一状态采取某一动作后得到的return期望。它的定义为：
$$
q_{\pi}(s,a)=\mathbb{E}[G_{t}|S_{t}=s,A_{t}=a]
$$
​		显然它依赖于策略$\pi$。它与state value有什么关系呢？显然有：
$$
\underbrace{\mathbb{E}[G_t|S_t=s]}_{v_\pi(s)}=\sum_a\underbrace{\mathbb{E}[G_t|S_t=s,A_t=a]}_{q_\pi(s,a)}\pi(a|s)
$$
​		因而有：
$$
v_\pi(s)=\sum_a\pi(a|s){q_\pi(s,a)}
$$
​		我们比较这个式子和之前的贝尔曼公式，可以发现：
$$
q_{\pi}(s,a)=\sum_{r}p(r|s,a)r+\gamma\sum_{s^{\prime}}p(s^{\prime}|s,a){v_{\pi}(s^{\prime})}
$$
​		这个公式告诉我们，如果我们知道了每一个状态的state value以及公式中必要的概率，我们就可以知道每一个状态每一个action的value。同样的，state value公式也告诉我们如果我们知道了每一个action value，也可以得到每个状态的state value。

​		这里有个误区就是如果给出的策略中，一个state可以采取的action只有若干，那么其他可能的、潜在的action的value能计算吗？这当然是可以的，实际上我们给出的策略可能是不好的，而action value就可以用来帮助我们作策略的改进。比如下面的例子：

<img src="https://s2.loli.net/2023/07/19/bR821DnkwPa47cr.png" alt="image-20230719180244165" style="zoom:67%;" />

​		对s1来说，虽然我们的策略只给出了a2这一动作，但是对于其他动作，我们也可以计算出action value（注意在我们的grid world中，碰到墙壁是弹回本状态）：
$$
\begin{gathered}
q_{\pi}(s_{1},a_{1}) =-1+\gamma v_{\pi}(s_{1}), \\
q_\pi(s_1,a_3) =0+\gamma v_{\pi}(s_{3}), \\
q_\pi(s_1,a_4) =-1+\gamma v_{\pi}(s_{1}), \\
q_{\pi}(s_{1},a_{5}) =0+\gamma v_{\pi}(s_{1}). 
\end{gathered}
$$

----

​		至此关于贝尔曼方程的基本概念介绍完毕，下节介绍贝尔曼最优方程。
