<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0"><title>论文阅读:离线强化学习中的扩散策略 | 雨白的博客小屋</title><meta name="author" content="Ameshiro"><meta name="copyright" content="Ameshiro"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="论文地址：[2208.06193] Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning (arxiv.org) 摘要​    离线强化学习旨在使用先前收集的静态数据集学习最优策略。标准的RL方法通常在这种情况下表现不佳，因为存在对于分布外动作（不满足于数据集分布）的函数近似误差。虽然">
<meta property="og:type" content="article">
<meta property="og:title" content="论文阅读:离线强化学习中的扩散策略">
<meta property="og:url" content="https://www.ameshiro77.cn/posts/1c6e6ef6.html">
<meta property="og:site_name" content="雨白的博客小屋">
<meta property="og:description" content="论文地址：[2208.06193] Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning (arxiv.org) 摘要​    离线强化学习旨在使用先前收集的静态数据集学习最优策略。标准的RL方法通常在这种情况下表现不佳，因为存在对于分布外动作（不满足于数据集分布）的函数近似误差。虽然">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2023/04/03/pvexKZFJ94oGbu8.jpg">
<meta property="article:published_time" content="2024-09-20T15:52:26.000Z">
<meta property="article:modified_time" content="2024-09-26T10:09:19.437Z">
<meta property="article:author" content="Ameshiro">
<meta property="article:tag" content="论文阅读">
<meta property="article:tag" content="强化学习">
<meta property="article:tag" content="生成式AI">
<meta property="article:tag" content="扩散模型">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2023/04/03/pvexKZFJ94oGbu8.jpg"><link rel="shortcut icon" href="https://s2.loli.net/2023/03/29/TFk1nsXWLMxqGa8.png"><link rel="canonical" href="https://www.ameshiro77.cn/posts/1c6e6ef6.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":50,"languages":{"author":"作者: Ameshiro","link":"链接: ","source":"来源: 雨白的博客小屋","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  }
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '论文阅读:离线强化学习中的扩散策略',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-09-26 18:09:19'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/custom.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/js-heo@1.0.11/mainColor/heoMainColor.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/js-heo@1.0.11/poem/poem.css"><meta name="generator" content="Hexo 5.4.2"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://s2.loli.net/2023/03/27/ec26kEVdfgCb7l9.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">48</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">31</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">11</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://s2.loli.net/2023/04/03/pvexKZFJ94oGbu8.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="雨白的博客小屋"><span class="site-name">雨白的博客小屋</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间线</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">论文阅读:离线强化学习中的扩散策略</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-09-20T15:52:26.000Z" title="发表于 2024-09-20 23:52:26">2024-09-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-09-26T10:09:19.437Z" title="更新于 2024-09-26 18:09:19">2024-09-26</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">1.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>6分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="论文阅读:离线强化学习中的扩散策略"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>论文地址：<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2208.06193#:~:text=A paper that proposes using diffusion">[2208.06193] Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning (arxiv.org)</a></p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​    离线强化学习旨在使用<strong>先前收集的静态数据集</strong>学习最优策略。标准的RL方法通常在这种情况下表现不佳，因为存在对于<strong>分布外动作（不满足于数据集分布）</strong>的函数近似误差。虽然已经提出了多种正则化方法来缓解这个问题，但它们通常受到表达能力有限的策略类别的限制。本文提出将策略表示为<strong>扩散模型</strong>，引入了扩散Q学习（Diffusion-QL,DQL），利用条件扩散模型来表示策略（条件即观测的状态）。本文学习一个动作价值函数，并且在条件扩散模型的训练损失中添加了一个最大化动作价值的项，使得损失寻求在behavior policy附近的最佳动作（即模仿学习，给出的数据集通常是expert dataset）。我们展示了基于扩散模型的策略的表达能力，以及在扩散模型下的行为克隆和策略改进的耦合，都有助于Diffusion-QL的卓越性能。</p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>​    离线强化学习旨在完全从先前收集的数据中学习有效的策略，而无需与环境交互。消除与环境在线交互的需求使得离线RL对于自动驾驶这种应用很有利，因为使用未训练的策略进行现实世界探索是风险高、成本高或耗时的。离线RL强调使用先前的数据（如人类演示），这些数据通常比在线交互的成本要低得多。然而，仅依赖先前收集的数据使得离线RL成为一个具有挑战性的任务，<strong>因为标准策略改进方法应用于离线数据集通常会导致模型依赖于评估在数据集中未见过的动作（即：没有用改进后的策略执行的action）</strong>，因此它们的价值不太可能被准确估计。因此，对于离线RL的简单方法通常学习到的策略性能不佳，<strong>倾向于偏好分布外动作</strong>，其价值被高估，导致不满意的性能。</p>
<p>​    先前关于离线RL的工作通常以四种方式之一解决这个问题：1.通过<strong>正则化</strong>来限制策略与行为策略的偏离程度；2.限制学习到的价值函数，使其对分布外动作赋予低价值；3.引入基于模型的方法（<strong>就是学习环境，现在基本都是model-free的，不用学习环境</strong>）；4.将离线RL视为带有reward指导的序列预测问题。<strong>本文分方法是第一类。</strong>从经验上看，策略正则化的离线RL方法的性能通常略逊于其他方法，这主要是因为策略正则化方法由于其有限的能力来准确表示行为策略，因此表现不佳。例如，策略正则化可能会将代理的探索空间限制在一个只有次优动作的小区域内，然后Q学习将被诱导收敛到一个次优策略。</p>
<p>​    策略正则化的不准确主要有两个原因：1.策略类别的表达能力不够；2.正则化方法不当。在大多数先前工作中，策略是由神经网络输出指定均值和对角协方差的高斯分布。然而，由于离线数据集通常是由多种策略混合收集的，这就使得不能很好地由对角高斯策略建模。为此，本文提出了一种使用扩散模型进行策略正则化的方法。具体来说，本文使用基于MLP的DDPM作为策略，构建了一个包含两个项的扩散损失目标：1.一个行为克隆项（<strong>behavior cloning，用于正则</strong>），鼓励扩散模型以与训练集相同的分布采样动作;2.一个策略改进项，试图采样高价值动作（根据学习到的Q值）。扩散模型是一个条件模型，以状态为条件，动作为输出。</p>
<p>​    <strong>为什么使用扩散模型？</strong>它有几个特性：首先，扩散模型非常具有表达能力，能够很好地捕捉<strong>多峰分布</strong>。其次，扩散模型损失构成了一种强大的分布匹配技术，因此它可以被视为一种强大的基于样本的策略正则化方法，无需额外的行为克隆。第三，扩散模型通过迭代细化进行生成，并且可以从最大化Q值函数的指导中，在每个逆向扩散步骤中添加。</p>
<h1 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h1><h2 id="关于RL中的扩散模型"><a href="#关于RL中的扩散模型" class="headerlink" title="关于RL中的扩散模型"></a>关于RL中的扩散模型</h2><p>​    有人提出通过表达性强且稳定的扩散模型更好地模仿人类行为。Diffuser将扩散模型作为轨迹生成器应用。该工作将状态-动作对的完整轨迹形成一个扩散模型的单个样本，然后学习一个单独的回报模型来预测每个轨迹样本的累积奖励。然后，<strong>回报模型的指导被注入到逆向采样阶段</strong>。</p>
<h1 id="DQL"><a href="#DQL" class="headerlink" title="DQL"></a>DQL</h1><h2 id="扩散策略"><a href="#扩散策略" class="headerlink" title="扩散策略"></a>扩散策略</h2><p>​    首先符号说明：使用上标 i∈{1,…,N}来表示扩散时间步，使用下标 t∈{1,…,T}来表示轨迹时间步。</p>
<p>​    通过条件扩散模型的逆向过程，将RL策略表示为：</p>
<script type="math/tex; mode=display">
\pi_\theta(a\mid s)=p_\theta(a^{0:N}\mid s)=\mathcal{N}(a^N;0,I)\prod_{i=1}^Np_\theta(a^{i-1}\mid a^i,s)</script><p>​    逆向过程的最后输出<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="2.185ex" height="1.909ex" role="img" focusable="false" viewBox="0 -833.9 965.6 843.9"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mn" transform="translate(562,363) scale(0.707)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></g></svg></mjx-container>就是用于RL评估的动作。首先采样一个服从于正态分布的<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.023ex;" xmlns="http://www.w3.org/2000/svg" width="2.805ex" height="1.937ex" role="img" focusable="false" viewBox="0 -846 1239.9 856"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msup"><g data-mml-node="mi"><path data-c="1D44E" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path></g><g data-mml-node="mi" transform="translate(562,363) scale(0.707)"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g></g></g></g></svg></mjx-container>，则通过重参数化的技巧，每步扩散结果为（1）：</p>
<script type="math/tex; mode=display">
a^{i-1}\mid a^{i}=\frac{a^{i}}{\sqrt{\alpha_{i}}}-\frac{\beta_{i}}{\sqrt{\alpha_{i}(1-\bar{\alpha}_{i})}}\epsilon_{\theta}(a^{i},s,i)+\sqrt{\beta_{i}}\epsilon</script><p>​    去噪模型就用MLP表示，优化目标（2）与DDPM相同：</p>
<script type="math/tex; mode=display">
\mathcal{L}_{d}(\theta)=\mathbb{E}_{i\sim\mathcal{U},\epsilon\sim\mathcal{N}(\mathbf{0},\boldsymbol{I}),(\boldsymbol{s},\boldsymbol{a})\sim\mathcal{D}}\left[||\epsilon-\epsilon_{\theta}(\sqrt{\bar{\alpha}_{i}}a+\sqrt{1-\bar{\alpha}_{i}}\epsilon,s,i)||^{2}\right]</script><p>​    这个扩散损失即一个behavior cloning损失，用来学习behavior policy。<strong>逆向扩散链的边缘分布提供了一个隐式的、表达性强的分布（因为不是直接给出概率密度，而是通过扩散过程逐渐生成的），能够捕捉离线数据集所表现出的复杂分布特性，如偏斜性和多峰性。且这种方法是基于采样的，我们不需要知道行为策略是什么。</strong></p>
<h2 id="Q学习"><a href="#Q学习" class="headerlink" title="Q学习"></a>Q学习</h2><p>​    只有Ld损失的话，我们不可能取得比behavior policy更好的策略。因此我们需要用Q-value去指导扩散过程，来让策略去采样更高价值的动作。因此最终的策略学习目标函数（3）是：</p>
<script type="math/tex; mode=display">
\pi=\arg\min\mathcal{L}(\theta)=\mathcal{L}_d(\theta)+\mathcal{L}_q(\theta)=\mathcal{L}_d(\theta)-\alpha\cdot\mathbb{E}_{\boldsymbol{s}\sim\mathcal{D},\boldsymbol{a}^0\sim\pi_\theta}\left[Q_\phi(s,\boldsymbol{a}^0)\right]</script><p>​    这里的Q函数就用一种传统方式学习，且用到一种double Q-learning的trick。这里用到两个Q学习网络和两个Q目标网络，然后优化下面目标函数（4）来更新参数：</p>
<script type="math/tex; mode=display">
\mathbb{E}_{(\boldsymbol{s}_t,\boldsymbol{a}_t,\boldsymbol{s}_{t+1})\sim\mathcal{D},\boldsymbol{a}_{t+1}^0\sim\pi_{\theta'}}\left[\left|\left|\left(r(s_t,a_t)+\gamma\min_{i=1,2}Q_{\phi'}(s_{t+1},a_{t+1}^0)\right)-Q_{\phi_i}(s_t,a_t)\right|\right|^2\right]</script><p>​    算法图：</p>
<p><img src="https://cdn.jsdelivr.net/gh/Ameshiro77/BlogPicture/pic/image-20240925005156589.png" alt="image"></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://www.ameshiro77.cn">Ameshiro</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://www.ameshiro77.cn/posts/1c6e6ef6.html">https://www.ameshiro77.cn/posts/1c6e6ef6.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.ameshiro77.cn" target="_blank">雨白的博客小屋</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/">论文阅读</a><a class="post-meta__tags" href="/tags/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/">强化学习</a><a class="post-meta__tags" href="/tags/%E7%94%9F%E6%88%90%E5%BC%8FAI/">生成式AI</a><a class="post-meta__tags" href="/tags/%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B/">扩散模型</a></div><div class="post_share"><div class="social-share" data-image="https://s2.loli.net/2023/04/03/pvexKZFJ94oGbu8.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/6f19da25.html" title="论文阅读:用于信息共享的AI生成激励机制和全双工语义通信"><img class="cover" src="https://s2.loli.net/2023/04/03/pvexKZFJ94oGbu8.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">论文阅读:用于信息共享的AI生成激励机制和全双工语义通信</div></div></a></div><div class="next-post pull-right"><a href="/posts/61bbf8ca.html" title="论文阅读-基于扩散模型的强化学习应用于边缘AIGC服务（的调度）"><img class="cover" src="https://s2.loli.net/2023/04/03/pvexKZFJ94oGbu8.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">论文阅读-基于扩散模型的强化学习应用于边缘AIGC服务（的调度）</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/61bbf8ca.html" title="论文阅读-基于扩散模型的强化学习应用于边缘AIGC服务（的调度）"><img class="cover" src="https://s2.loli.net/2023/04/03/pvexKZFJ94oGbu8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-12</div><div class="title">论文阅读-基于扩散模型的强化学习应用于边缘AIGC服务（的调度）</div></div></a></div><div><a href="/posts/c3cb86a7.html" title="论文阅读:促进DRL，生成式扩散模型在网络优化的应用"><img class="cover" src="https://s2.loli.net/2023/04/03/pvexKZFJ94oGbu8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-25</div><div class="title">论文阅读:促进DRL，生成式扩散模型在网络优化的应用</div></div></a></div><div><a href="/posts/6f19da25.html" title="论文阅读:用于信息共享的AI生成激励机制和全双工语义通信"><img class="cover" src="https://s2.loli.net/2023/04/03/pvexKZFJ94oGbu8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-22</div><div class="title">论文阅读:用于信息共享的AI生成激励机制和全双工语义通信</div></div></a></div><div><a href="/posts/6a56c53.html" title="论文阅读-基于扩散模型先验生成器的低复杂度MIMO信道估计"><img class="cover" src="https://s2.loli.net/2023/04/03/pvexKZFJ94oGbu8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-09-02</div><div class="title">论文阅读-基于扩散模型先验生成器的低复杂度MIMO信道估计</div></div></a></div><div><a href="/posts/4558e1e3.html" title="边缘计算与强化学习简述"><img class="cover" src="https://s2.loli.net/2023/04/03/pvexKZFJ94oGbu8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-15</div><div class="title">边缘计算与强化学习简述</div></div></a></div><div><a href="/posts/10dad796.html" title="论文阅读:QECO-移动边缘计算(MCE)中基于DRL的面向QoE的计算卸载算法"><img class="cover" src="https://s2.loli.net/2023/04/03/pvexKZFJ94oGbu8.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-10-08</div><div class="title">论文阅读:QECO-移动边缘计算(MCE)中基于DRL的面向QoE的计算卸载算法</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://s2.loli.net/2023/03/27/ec26kEVdfgCb7l9.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Ameshiro</div><div class="author-info__description">上海黄渡理工职业技术学校软件人一枚</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">48</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">31</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">11</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/Ameshiro77" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:512065954@qq.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">准备开强化学习和我CV课程章节性内容的笔记分类了..评论区布置了，但只是布置了，还请别做一些恶意评论的事情 另外换了个字体 音乐播放器也能用了~</div></div><div class="card-widget" id="card-poem"><div id="poem_sentence"></div><div id="poem_info"><div id="poem_dynasty"></div><div id="poem_author"></div></div></div><script src="https://cdn.jsdelivr.net/npm/js-heo@1.0.11/poem/jinrishici.js" charset="utf-8"></script><script type="text/javascript">jinrishici.load(function(result) {
var sentence = document.querySelector("#poem_sentence")
var author = document.querySelector("#poem_author")
var dynasty = document.querySelector("#poem_dynasty")

var sentenceText = result.data.content
sentenceText = sentenceText.substr(0, sentenceText.length - 1);
sentence.innerHTML = sentenceText
dynasty.innerHTML = result.data.origin.dynasty
author.innerHTML = result.data.origin.author + '《' + result.data.origin.title + '》'
});</script><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%91%98%E8%A6%81"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AE%80%E4%BB%8B"><span class="toc-number">2.</span> <span class="toc-text">简介</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E7%BD%AE%E7%9F%A5%E8%AF%86"><span class="toc-number">3.</span> <span class="toc-text">前置知识</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B3%E4%BA%8ERL%E4%B8%AD%E7%9A%84%E6%89%A9%E6%95%A3%E6%A8%A1%E5%9E%8B"><span class="toc-number">3.1.</span> <span class="toc-text">关于RL中的扩散模型</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#DQL"><span class="toc-number">4.</span> <span class="toc-text">DQL</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%A9%E6%95%A3%E7%AD%96%E7%95%A5"><span class="toc-number">4.1.</span> <span class="toc-text">扩散策略</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Q%E5%AD%A6%E4%B9%A0"><span class="toc-number">4.2.</span> <span class="toc-text">Q学习</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/4558e1e3.html" title="边缘计算与强化学习简述"><img src="https://s2.loli.net/2023/04/03/pvexKZFJ94oGbu8.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="边缘计算与强化学习简述"/></a><div class="content"><a class="title" href="/posts/4558e1e3.html" title="边缘计算与强化学习简述">边缘计算与强化学习简述</a><time datetime="2024-10-14T16:17:34.000Z" title="发表于 2024-10-15 00:17:34">2024-10-15</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/10dad796.html" title="论文阅读:QECO-移动边缘计算(MCE)中基于DRL的面向QoE的计算卸载算法"><img src="https://s2.loli.net/2023/04/03/pvexKZFJ94oGbu8.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="论文阅读:QECO-移动边缘计算(MCE)中基于DRL的面向QoE的计算卸载算法"/></a><div class="content"><a class="title" href="/posts/10dad796.html" title="论文阅读:QECO-移动边缘计算(MCE)中基于DRL的面向QoE的计算卸载算法">论文阅读:QECO-移动边缘计算(MCE)中基于DRL的面向QoE的计算卸载算法</a><time datetime="2024-10-07T16:19:49.000Z" title="发表于 2024-10-08 00:19:49">2024-10-08</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2024 By Ameshiro</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'bTLIPsGcteUIZJeAkH357F5o-gzGzoHsz',
      appKey: 'foxd3zMb0pek7Hd5ZjgzpmTv',
      avatar: 'mp',
      serverURLs: 'https://btlipsgc.lc-cn-n1-shared.com',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: false
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script src="./js/jquery.js"></script><script src="./js/foot.js"></script><script src="http://cdn.bootcss.com/pace/1.0.2/pace.min.js" async></script><script defer src="https://cdn.jsdelivr.net/combine/npm/jquery@latest/dist/jquery.min.js,gh/weilining/jsdelivr/jquery/circlemagic/circlemagic.min.js,gh/weilining/jsdelivr@latest/jquery/circlemagic/butterflycirclemagic.js"></script><div class="aplayer no-destroy" data-id="7907804893" data-server="netease" data-type="playlist" data-fixed="true" data-mini="true" data-listFolded="false" data-order="random" data-preload="none" data-autoplay="false" muted></div><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/metingjs/dist/Meting.min.js"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></body></html>