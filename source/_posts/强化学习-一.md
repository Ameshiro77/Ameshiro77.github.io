---
uuid: 930355d0-edbb-11ed-8310-d1886ba47c07
title: 强化学习(一):基本概念
author: Ameshiro
top: false
toc: true
mathjax: true
copyright_author: Ameshiro
description: 强化学习学习笔记
cover: 'https://s2.loli.net/2023/05/09/gaVpREojIX8LZDO.jpg'
tags:
  - 强化学习
  - 智能体
categories:
  - 强化学习
abbrlink: aa0fa061
date: 2023-05-09 00:15:39
updated:
img:
top_img:
password:
summary:
copyright:
copyright_author_href:
copyright_url:
copyright_info:
aplayer:
highlight_shrink:
aside:
---

​		从本文开始，均是针对哔哩哔哩网课的笔记记录。本文参考：[【强化学习的数学原理】课程：从零开始到透彻理解（完结）_哔哩哔哩_bilibili](https://www.bilibili.com/video/BV1sd4y167NS/?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click&vd_source=05dba3ddf58ccccdd5db01aec71596cc)

​		本文是对基本概念的介绍。

# 笔记背景范例

​		本课程的背景例子如图：

<img src="https://s2.loli.net/2023/07/11/KDlXiugmkqw4YMj.png" alt="image-20230711105145024" style="zoom:50%;" />

​		这是一个网格世界，我们的机器人要从给定的起点出发，寻找一个合适的路径到达我们的目的地(也就是我们的task)。机器人只能向相邻四个方格移动，且整个网格世界存在边界。我们的机器人要尽可能避免进入forbidden，且不能超越边界。    以此背景，我们介绍下面概念。

# State：状态

​		**state**是指智能体相对于环境所处的状态。在我们的例子中，不同的方格代表了不同的状态，是一个包含x，y二维坐标信息的向量。如图所示：

<img src="https://s2.loli.net/2023/07/11/pVcWwRKzOIhYvyZ.png" alt="image-20230711105726622" style="zoom:50%;" />

​		state space:即状态空间，也就是所有状态的集合:$\mathcal{S}=\{s_i \} _{i=1}^{9}$.   (打latex真累。。)

# Action：行动

​		**action**是指对于每一个状态，智能体拥有的一系列可以采取的行动。对我们的例子来说，共有如下五个状态：

<img src="https://s2.loli.net/2023/07/11/D21RaXl9OKnPjZm.png" alt="image-20230711110637375" style="zoom:50%;" />

​		a1~a5依次是向上、向右、向下、向左已经保持不动。

​		**Action space of a state**： 某个状态的动作空间。需要注意，不同的状态，动作空间可能是不一样的。公式表示为:$\mathcal{A}(s_i)=\{ a_i \} _{i=1}^{5}$ . 可以看出，action是依赖于state的。

# State transition：状态迁移

​		当采取一个action时，智能体可能会从一个state变到另一个state，这就叫做state transition。它实际上是定义智能体和环境交互的一种行为，我们可以（在本例子的游戏环境中）任意定义这一行为。

​		比如说，我们在s1状态采取行动a1，也就是撞到边界，我们可以定义智能体保持不动: $s1\overset{a1}{\longrightarrow}s1$；也可以定义会被撞到身后的格子: $s1\overset{a1}{\longrightarrow}s4$ .

​		又或者，当我们撞到forbidden area时，我们有两种方案：要么进不去原地不动，要么能进去但是会受到惩罚(penalty)。普适地，我们选择后者方案，因为agent冒险进入forbidden area的话或许会能得到更好的通往target的路径。

​		我们可以得到如下table：

<img src="https://s2.loli.net/2023/07/11/csEnw4VLmihZUpB.png" alt="image-20230711193655918" style="zoom:67%;" />

​		但是，通过这张表**无法表达转换成多个状态的可能性**。因此，我们采用***State transition probability***来从数学上表述；对于这种确定性案例(**deterministic** case)，我们用数学语言表述为：
$$
\begin{gather}
p(s2|s1,a2)=1  \\
p(s_i|s1,a2)=0， \forall{i}\neq2 \\
\end{gather}
$$
​		注意，是一个条件是s和a的条件概率，而不是联合概率。当然，state transition也可以是随机的(**stochastic**)。比如：
$$
\begin{gather}
p(s2|s1,a2)=0.5  \\
p(s_5|s1,a2)=0.5\\
\end{gather}
$$

# Policy：策略

​		policy是强化学习独有的概念，它用来告诉智能体在当前状态该采取什么行动。如图，这些箭头就表示了一个策略：

<img src="https://s2.loli.net/2023/07/11/qM6JzRoScA1OXth.png" alt="image-20230711235246536" style="zoom:67%;" />

​		当然，我们需要一种更数学的方法去表示。在强化学习中，我们使用条件概率，用$\pi$来表示策略。例如，对于状态$s1$,有：
$$
\begin{gather}
\pi(a_1|s_1)=0 \hspace{1em} \pi(a_2|s_1)=1 \hspace{1em} \pi(a_3|s_1)=0 \\
\pi(a_4|s_1)=0 \hspace{1em} \pi(a_5|s_1)=0
\end{gather}
$$
​		这是一个deterministic policy。同样的，我们有stochastic policy，例如：
$$
\begin{gather}
\pi(a_1|s_1)=0 \hspace{1em} \pi(a_2|s_1)=0.5 \hspace{1em} \pi(a_3|s_1)=0.5\\
\pi(a_4|s_1)=0 \hspace{1em} \pi(a_5|s_1)=0
\end{gather}
$$
​		表示在图里，就是这个样子：

<img src="https://s2.loli.net/2023/07/12/y89lnIi5F7qHTM2.png" alt="image-20230712000206306" style="zoom: 50%;" />

​		我们可以用一张表来描述policy：

<img src="https://s2.loli.net/2023/07/12/qKZrHXLbkioBtJW.png" alt="image-20230712000451123" style="zoom:67%;" />

​		注意到，这张表可以即表示deterministic，又表示stochastic，而不同于上面的状态迁移表只能表示deterministic。实际编程中，我们可以用数组或矩阵表示这样一个策略。

# Reward & Return：奖励 & 累积回报

​		Reward是RL中独有的概念，是指采取一个行动后智能体得到的一个实数，一个标量。正值表示鼓励这样的行为，负值表示作出惩罚。**正值也可以表示punishment。**

​		在我们的范例中，我们可以如下设计reward：

1.智能体尝试越出边界，则$r_{bound}=-1$

2.智能体尝试进入forbidden，则$r_{forbid}=-1$

3.智能体到达target，则$r_{target}=1$

4.otherwise  ,  $r=0$

​		我们同样可以用一张表格描述reward：

<img src="https://s2.loli.net/2023/07/12/IOzwqBE6kHuYK1V.png" alt="image-20230712102641484" style="zoom:67%;" />

​		但是这种只能表示deterministic cases，我们依旧使用条件概率来表示stochastic cases。比如，如果我们努力学习，我们就会获得reward，但是我们得到多少是不确定的：$p(r=1|s_1,a_1)=0.5$ , $p(r=0|s_1,a_1)=0.5$.

​		一个**trajectory**是一个状态-行为-奖励链，比如:
$$
s_1\underset{r=0}{\overset{a_2}{\longrightarrow}}s_2\underset{r=0}{\overset{a_3}{\longrightarrow}}s_5\underset{r=0}{\overset{a_3}{\longrightarrow}}s_8\underset{r=1}{\overset{a_2}{\longrightarrow}}s_9
$$
​		而一个**return**则是某个trajectory获得的reward的总和。不同的policy给出不同的trajectory，进而会给出不同的return。**Return用来评估policy。我们希望获得return更大的policy。**

​		这样的话会有一个问题：加入我们的trajectory是无限的，它在target处一直原地转圈，如下：
$$
s_1\underset{r=0}{\overset{a_2}{\longrightarrow}}s_2\underset{r=0}{\overset{a_3}{\longrightarrow}}s_5\underset{r=0}{\overset{a_3}{\longrightarrow}}s_8\underset{r=1}{\overset{a_2}{\longrightarrow}}s_9\underset{r=1}{\overset{a_5}
{\longrightarrow}}s_9……
$$
​		这样的话，我们就会得到return最后是1+1+1+1……就变成了∞。这样的定义显然是不合适的，因此我们需要引进一个衰减系数（**discount rate**）：$\gamma \in [0,1)$。这样，我们就会得到一个Discounted return：

$$
\begin{aligned}
\text { discounted return } & =0+\gamma 0+\gamma^{2} 0+\gamma^{3} 1+\gamma^{4} 1+\gamma^{5} 1+\ldots \\
& =\gamma^{3}\left(1+\gamma+\gamma^{2}+\ldots\right)=\gamma^{3} \frac{1}{1-\gamma}
\end{aligned}
$$
​		这样一来，我们就得到了有穷的return，并且平衡了近处和远处的rewards。**如果$\gamma$趋近于0，那么return更受近处未来的reward决定；反之如果趋近于1，则return更受远处未来的reward决定**。

# Episode：回合/轮次

​		当我们遵循一个policy与environment交互时，智能体可能在一些终止状态（terminal states）停下来。由此得到的trajectory被称为一个episode（也可以叫一个trial）。可以说一个episode可以被描述为一个有穷的trajectory。如图便是一个episode。

<img src="https://s2.loli.net/2023/07/15/GJdlhs2yYfnva9M.png" alt="image-20230715000226348" style="zoom:67%;" />

​		然而有些任务是没有terminal states的。这意味着我们与环境的交互无法终止。这样的任务叫做continuing tasks。事实上，我们可以用一个统一的数学方式，把episodic tasks转化为continuing tasks。以本案例为例，我们有两种做法：

1.把target state看成一个特殊的absorbing state，只要智能体进入这个state，就永远不会离开，之后reward都是0.

2.把target state看成一个普通的state。智能体可以选择离开，也可以在再次进入后获得+1的reward。

​		本案例中我们选择第二个方法，这就就可以方便把target state看成一个normal state。

# Markov decision process（MDP）:马尔科夫决策过程

​		几乎所有强化学习问题都可以转化为MDP，它是强化学习问题的理论基础。马尔科夫是一个无记忆的随机过程，可以用一个元组$<S,P>$表示，其中$S$是有限数量的状态集合，$P$是状态转移概率矩阵。给定一个policy，MDP就可以变成一个Markov process，如下图所示：

<img src="https://s2.loli.net/2023/07/15/B24jXvqAw9Zf3rb.png" alt="image-20230715002528078" style="zoom:67%;" />

​		具体来说，MDP包括以下关键元素。

· Sets（集合）：

1. State：状态集合 $S$
2. Action：行动集合 $A(S)$
3. Reward： 奖励集合 $R(s,a)$

· Probability distribution（概率分布）：

1. state transition probability： $p(s'|s,a)$
2. reward probability： $p(r|s,a)$

· Policy：$\pi(a|s)$

· Markov property：无记忆性
$$
\begin{array}{l}
p\left(s_{t+1} \mid a_{t}, s_{t}, \ldots, a_{0}, s_{0}\right)=p\left(s_{t+1} \mid a_{t}, s_{t}\right) \\
p\left(r_{t+1} \mid a_{t}, s_{t}, \ldots, a_{0}, s_{0}\right)=p\left(r_{t+1} \mid a_{t}, s_{t}\right)
\end{array}
$$
