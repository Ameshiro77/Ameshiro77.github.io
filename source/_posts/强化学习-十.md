---
uuid: adad9710-41a8-11ee-bd16-475a1bd85c43
title: 强化学习(十):Actor-Critic方法
author: Ameshiro
top: false
cover: 'https://s2.loli.net/2023/04/03/pvexKZFJ94oGbu8.jpg'
toc: true
mathjax: true
copyright_author: Ameshiro
hidden: false
abbrlink: d22c08ed
date: 2023-08-21 19:32:01
updated:
description:
img:
top_img:
password:
summary:
tags:
  - 强化学习
  - AC算法
categories:
  - 强化学习
copyright:
copyright_author_href:
copyright_url:
copyright_info:
aplayer:
highlight_shrink:
aside:
---

​			这是当下RL最流行的算法之一。它把基于value的方法，特别是值函数近似，引入到了policy gradient中，便得到了AC。AC方法仍然是一种PG方法。actor和critic分别代表什么意思呢？这里，actor指policy update，因为策略是用来采取行动的；critic代表policy evaluation或value estimation，是用来估计s和q的value的，以评价策略好坏。如果要引入deterministic的策略，就得到了最后的DPG算法。

# QAC

​		先来介绍最简单的AC算法，以此学习AC算法核心思想。回顾我们的SGD下的PG算法：
$$
\theta_{t+1}=\theta_t+\alpha\nabla_\theta\ln\pi(a_t|s_t,\theta_t){q_t(s_t,a_t)}
$$
​		因为参数在变化，所以这个算法就是在更新策略，那么这个算法其实就是actor。那么critic是啥呢？显然就是对$q_t(s_t,a_t)$的估计。我们之前提过用MC估计q值，如果用TD方法估计的话，这个算法就一般被称为AC。QAC算法如下：

<img src="https://s2.loli.net/2023/08/24/if4bz926VWURLex.png" alt="image-20230824005132131" style="zoom:67%;" />

​		我们得到$(s_t,a_t,r_{t+1},s_{t+1},a+{t+1})$实际就是一个experience sample，就可以用他们去估计action value；这就是我们先前讲过的结合了值函数近似的sarsa算法，也就是AC里的critic。

​		PG算法是on-policy的，而且我们不用变成epsilon-greedy的，因为ln的存在使得π值必须大于0，所以它本身就是stochastic的。

# A2C

​		为什么叫A2C呢？因为这个算法叫Advantage actor-critic。A2C是QAC的一个推广，基本思想是引入一个baseline(这里是“偏置”)来减少估计的方差。首先要介绍一个性质：我们所推导的策略梯度，如果引入一个新的偏置，是不会发生变化的。写成公式就是这个样子：
$$
\begin{aligned}
\nabla_{\theta}J(\theta)& =\mathbb{E}_{S\sim\eta,A\sim\pi}\left[\nabla_{\theta}\ln\pi(A|S,\theta_{t})q_{\pi}(S,A)\right]  \\
&=\mathbb{E}_{S\sim\eta,A\sim\pi}\Big[\nabla_\theta\ln\pi(A|S,\theta_t)(q_\pi(S,A)-{\color{blue}{b(S)}})\Big]
\end{aligned}
$$
​		这里的b(S)是一个标量函数，作为我们的baseline。为什么引入b不会变化？它有什么用？首先考虑前者。要证明不会变化，根据期望的线性关系，就是证明：
$$
\mathbb{E}_{S\sim\eta,A\sim\pi}\Big[\nabla_{\theta}\ln\pi(A|S,\theta_{t})b(S)\Big]=0
$$
​		我们按照定义，把它展开，一步步推导：
$$
\begin{aligned}
\mathbb{E}_{S\sim\eta,A\sim\pi}\left[\nabla_{\theta}\ln\pi(A|S,\theta_{t})b(S)\right]& \begin{aligned}=\sum_{s\in\mathcal{S}}\eta(s)\sum_{a\in\mathcal{A}}\pi(a|s,\theta_t)\nabla_\theta\ln\pi(a|s,\theta_t)b(s)\end{aligned}  \\
&=\sum_{s\in\mathcal{S}}\eta(s)\sum_{a\in\mathcal{A}}\nabla_\theta\pi(a|s,\theta_t)b(s) \\
&=\sum_{s\in\mathcal{S}}\eta(s)b(s)\sum_{a\in\mathcal{A}}\nabla_\theta\pi(a|s,\theta_t) \\
&=\sum_{s\in\mathcal{S}}\eta(s)b(s)\nabla_\theta\sum_{a\in\mathcal{A}}\pi(a|s,\theta_t) \\
&\begin{aligned}=\sum_{s\in\mathcal{S}}\eta(s)b(s)\nabla_\theta1=0\end{aligned}
\end{aligned}
$$
​		这里的$\eta$是s的分布。可以看出也是因为这个b和a没关系，所以不会改变期望，因为可以从求和内提出去。

​		下面回答另一个问题：有什么用呢？为了方便表示，我们令：
$$
X(S,A)\doteq\nabla_{\theta}\ln\pi(A|S,\theta_{t})[q(S,A)-b(S)]
$$
​		这样梯度就可以写成$\nabla_{\theta}J(\theta)=\mathbb{E}[X]$。我们已经知道，b(S)不管咋变，E(X)是不会变的，但是var(X)（也就是概统里的方差D(X)）是会变的。首先引入一点。我们先前学的概统没有涉及到矩阵随机变量。如果X是个矩阵，那么我们有：
$$
\operatorname{tr}[\operatorname{var}(X)]=\mathbb{E}[X^TX]-\bar{x}^T\bar{x}
$$
​		我们用迹来作为评价方差的工具，这里的${\bar{x}}\doteq\mathbb{E}[X]$。这个公式的推导根据定义即可：
$$
\begin{aligned}
\operatorname{tr}[\operatorname{var}(X)]& =\mathrm{tr}\mathbb{E}[(X-\bar{x})(X-\bar{x})^{T}]  \\
&=\mathrm{tr}\mathbb{E}[XX^T-\bar{x}X^T-X\bar{x}^T+\bar{x}\bar{x}^T] \\
&=\mathrm{tr}\mathbb{E}[XX^{T}-\bar{x}X^{T}-X\bar{x}^{T}+\bar{x}\bar{x}^{T}] \\
&=\mathbb{E}[X^{T}X-X^{T}\bar{x}-\bar{x}^{T}X+\bar{x}^{T}\bar{x}] \\
&=\mathbb{E}[X^{T}X]-\bar{x}^{T}\bar{x}.
\end{aligned}
$$
​		显然$\bar{x}^T\bar{x}$是保持不变的，但$\mathbb{E}[X^TX]$可能会受到baseline的影响。为说明此，我们可以计算一下它：
$$
\begin{gathered}
\mathbb{E}[X^{T}X] =\mathbb{E}\left[(\nabla_\theta\ln\pi)^T(\nabla_\theta\ln\pi)(q_\pi(S,A)-b(S))^2\right] \\
=\mathbb{E}\left[\|\nabla_\theta\ln\pi\|^2(q_\pi(S,A)-b(S))^2\right] 
\end{gathered}
$$
​		由此看出baseline对方差有影响。因此我的目标就是找到一个最好的baseline让方差达到最小。那为什么要让方差达到最小？这是希望在采样的时候得到最小的误差。之前介绍的REINFORCE和QAC的baseline都是等于0的，但是这不是一个很好的baseline。最好的baseline应该是对任意的s，都有如下等式：
$$
b^*(s)=\frac{\mathbb{E}_{A\sim\pi}[\|\nabla_\theta\ln\pi(A|s,\theta_t)\|^2q_\pi(s,A)]}{\mathbb{E}_{A\sim\pi}[\|\nabla_\theta\ln\pi(A|s,\theta_t)\|^2]}
$$
​		这其实是对$\nabla_{b}\mathbb{E}[X^{T}X]=0$的解，具体过程就不提及了。这个公式里的$\|\nabla_\theta\ln\pi(A|s,\theta_t)\|^2$可以看作是一个权重。这个b虽然是最优的，但是太复杂了，实际中我们经常把权重去掉，直接求q的期望，那显然我们的b这时候就变成了**v(s)**：
$$
b(s)=\mathbb{E}_{A\sim\pi}[q_\pi(s,A)]=v_\pi(s)
$$
​		我们已经介绍了baseline的基本性质，下面看看怎么把它用到AC算法中。我们设定:$b(s)=v_\pi(s)$，那么梯度上升算法就变成了：
$$
\begin{aligned}
\theta_{t+1}& =\theta_t+\alpha\mathbb{E}\Big[\nabla_\theta\ln\pi(A|S,\theta_t)[q_\pi(S,A)-v_\pi(S)]\Big]  \\
&\dot{=}\theta_t+\alpha\mathbb{E}\Big[\nabla_\theta\ln\pi(A|S,\theta_t){\color{blue}{\delta_\pi(S,A)}}\Big]
\end{aligned}
$$
​		这里的$\delta_\pi(S,A)\doteq q_\pi(S,A)-v_\pi(S)$，它有个名字叫做advantage function，常用的表示符号是A，这里因为有A了所以用$\delta$表示。它为什么叫优势函数呢？根据$v_\pi(s)=\sum_a\pi(a|s){q_\pi(s,a)}$这一定义，我们可以把$v_\pi$看作是$q_\pi$在某一状态下的一个平均值。那如果某一个action比这个平均值大，那就说明它比较好，有一定优势，所以叫优势函数。那很自然的，我们可以得到这个算法的通过采样的stochastic version:
$$
\begin{aligned}
\theta_{t+1}=\theta_t+\alpha\nabla_\theta\ln\pi(a_t|s_t,\theta_t)\delta_t(s_t,a_t)
\end{aligned}
$$
​		类似于上节课，我们可以把式子重新组织为：
$$
\begin{aligned}
\theta_{t+1}& =\theta_{t}+\alpha\nabla_{\theta}\operatorname{ln}\pi(a_{t}|s_{t},\theta_{t})\delta_{t}(s_{t},a_{t})  \\
&=\theta_{t}+\alpha\frac{\nabla_{\theta}\pi(a_{t}|s_{t},\theta_{t})}{\pi(a_{t}|s_{t},\theta_{t})}\delta_{t}(s_{t},a_{t}) \\
&=\theta_{t}+\alpha\underbrace{\left(\frac{\delta_{t}(s_{t},a_{t})}{\pi(a_{t}|s_{t},\theta_{t})}\right)}_{\text{step size}}\nabla_{\theta}\pi(a_{t}|s_{t},\theta_{t})
\end{aligned}
$$
​		显然$\delta_t$比上节课的$q_t$要好，因为我们更在乎相对值而不是绝对值。更长远的，我们可以用TD error的形式来估计优势函数：
$$
\delta_t=q_t(s_t,a_t)-v_t(s_t)\rightarrow r_{t+1}+\gamma v_t(s_{t+1})-v_t(s_t)
$$
​		为什么这样是合理的？因为从期望的角度来看，我们有：
$$
\mathbb{E}[q_{\pi}(S,A)-v_{\pi}(S)|S=s_{t},A=a_{t}]=\mathbb{E}\Big[R+\gamma v_{\pi}(S^{\prime})-v_{\pi}(S)|S=s_{t},A=a_{t}\Big]
$$
​		这样做的好处是，对于$\delta_t=q_t(s_t,a_t)-v_t(s_t)$，我们要两个神经网络来近似q和v，但是现在只需要一个神经网络近似v就可以了。到此，我们就得到了A2C的算法：

<img src="https://s2.loli.net/2023/08/25/saZtT4zSnLoX1Rv.png" alt="image-20230824114054452" style="zoom:67%;" />

​		可以看出这是一个on-policy的算法。因为π是stochastic的，所以也不用epsilon-greedy。

# Off-policy actor-critic

​		如果我们之前有一些经验了，我们想用这些经验，我们就要用off policy的方法。我们将会介绍重要性采样这一方法，来把on-policy转化成off-policy的。我们之前说过，PG是on-policy的，是因为它的梯度是一个期望，这个期望中的A要服从策略π，它即是behavior又是target。我们为什么要转化成off-policy？因为我们希望复用一些其他方法得到的经验。

​		我们转化的工具就是importance sampling。实际上，任何要求期望的算法，都可以用到重要性采样的技术，包括之前的MC和TD。我们先看一些直观的例子：假设有个随机变量X属于{+1,-1}，服从一个分布$p_0$：$p(X=+1)=p(X=-1)=0.5$。这时显然期望就是0.但是如果我们不知道分布$p_0$的表达式，那我们能不能通过采样的方式求期望呢？

​		讲MC的时候，我们提到过大数定律相关的方法。现在我们考虑一种新的情况：我们有一些样本，它们是在$p_1$而非$p_0$下采样的。假设$p_1$分布为：$p(X=+1)=0.8，p(X=-1)=0.2$;那显然E(X)=0.6，与之前的是不相等的。我们的问题是，我们想用$p_1$产生的采样来估计$p_0$下的期望。我们这样做的原因是因为，off-policy里有一个behavior policy $\beta$,就是我们产生数据的$p_1$；还有个target policy $\pi$，作为我们的$p_0$。

## Importance Sampling

​		我们的核心公式是：
$$
\mathbb{E}_{X\sim p_0}[X]=\sum_{x}p_0(x)x=\sum_{x}{p_1(x)}\underbrace{\frac{p_0(x)}{p_1(x)}x}_{f(x)}=\mathbb{E}_{X\sim p_1}[f(X)]
$$
​		这就是重要性采样最核心的方法；我们拥有在$p_1$下的采样。怎么求$\mathbb{E}_{X\sim p_1}[f(X)]$呢？不难，我们只需要令：
$$
\bar{f}\doteq\frac{1}{n}\sum_{i=1}^{n}f(x_{i}),\quad\text{where }x_{i}\sim p_{1}
$$
​		因此根据大数定理就有：
$$
\begin{gathered}
\mathbb{E}_{X\sim p_{1}}[\bar{f}] =\mathbb{E}_{X\sim p_1}[f(X)] \\
\operatorname{var}_{X\sim_{p_{1}}}[\bar{f}] =\frac1n\mathrm{var}_{X\sim p_1}[f(X)] 
\end{gathered}
$$
​		进而我们得到了我们最终的算法：
$$
{\mathbb{E}_{X\sim p_0}[X]}\approx\bar{f}=\frac1n\sum_{i=1}^nf(x_i){=\frac1n\sum_{i=1}^n\frac{p_0(x_i)}{p_1(x_i)}x_i}
$$
​		这个式子实际上是一个加权平均是，权重$\frac{p_{0}(x_{i})}{p_{1}(x_{i})}$叫做importance weight。如果$p_{1}(x_{i})=p_{0}(x_{i})$,那么显然$\bar{f}$就成了$\bar{x}$；如果$p_{1}(x_{i})\geq p_{0}(x_{i})$，说明在$p_0$下比在$p_1$下更容易采样到x；所以要给一个重要性权重。

​		有同学可能会问：既然我们的计算需要知道$p_0$，那既然知道了，为什么不直接求期望呢？这是因为有时候我们有时候是确实不能直接求的，就比如考虑连续的情况，这时候期望是积分，如果$p_0$太复杂就求不出来了；再比如假如是个神经网络，我们根本无从知道$p_0$的表达式，就根本没法求。

## off-policy PG

​		有了重要性采样，我们把他用到PG中来实现off-policy学习。假设$\beta$是behavior policy，用来生成经验；我们的目标是用这些经验更新target policy π，π的参数是$\theta$，我们要优化这样的目标函数：
$$
J(\theta)=\sum_{s\in\mathcal{S}}d_\beta(s)v_{\pi}(s)=\mathbb{E}_{S\sim d_\beta}[v_{\pi}(S)]
$$
​		利用重要性采样，我们的梯度可以求得（细节略）：
$$
\nabla_\theta J(\theta)=\mathbb{E}_{S\sim\rho,A\sim\beta}\left[
{\color{blue}\frac{\pi(A|S,\theta)}{\beta(A|S)}}\nabla_\theta\ln\pi(A|S,\theta)q_\pi(S,A)\right]
$$
​		可以看出，这是的A服从的不是π而是$\beta$；期望内部与之前的差别就在于多了个重要性采样的权重（蓝色部分），分母就对应那个$p_0$，分母就对应那个$p_1$。

## off-policy AC算法

​		有了off-policy PG表达式，就可以应用到梯度上升里进行优化。off-policy里仍然可以加上baseline。我们依旧可以选择baseline为：$b(S)=v_{\pi}(S)$。这样就可以得到：
$$
\nabla_\theta J(\theta)=\mathbb{E}\left[\frac{\pi(A|S,\theta)}{\beta(A|S)}\nabla_\theta\ln\pi(A|S,\theta)\big(q_\pi(S,A)-v_\pi(S)\big)\right]
$$
​		此时的随机梯度下的算法就可以写为：
$$
\theta_{t+1}=\theta_t+\alpha_\theta\frac{\pi(a_t|s_t,\theta_t)}{\beta(a_t|s_t)}\nabla_\theta\ln\pi(a_t|s_t,\theta_t)\big(q_t(s_t,a_t)-v_t(s_t)\big)
$$
​		与A2C一样，我们用优势函数去近似代替：
$$
q_t(s_t,a_t)-v_t(s_t)\approx r_{t+1}+\gamma v_t(s_{t+1})-v_t(s_t)\doteq\delta_t(s_t,a_t)
$$
​		就得到了最后的算法：
$$
\theta_{t+1}=\theta_t+\alpha_\theta\frac{\pi(a_t|s_t,\theta_t)}{\beta(a_t|s_t)}\nabla_\theta\ln\pi(a_t|s_t,\theta_t)\delta_t(s_t,a_t)
$$
​		这里，lnπ的梯度就和分子的π约掉了，于是我们得到了最终版本：
$$
\theta_{t+1}=\theta_t+\alpha_\theta\left(\frac{\delta_t(s_t,a_t)}{\beta(a_t|s_t)}\right)\nabla_\theta\pi(a_t|s_t,\theta_t)
$$
​		但是注意一下，这里我们的分母是固定的值$\beta$，所以我们这里就没有探索了，只剩下exploitation了。off-policy AC和A2C伪代码基本差不多：

<img src="https://s2.loli.net/2023/08/25/yWm1Al4SuoMQnjr.png" alt="image-20230825014439707" style="zoom:67%;" />

# DPG

​		我们之前提到的QAC,A2C,Off-policy AC的π都是严格大于0的，即策略都是stochastic的。现在我们要使用deterministic policies。为什么要关心这样的策略？这是因为如果action有无限个的话，这种方式就不能表示了（神经网络没法输出无限个π值的，对吧？）。现在，我们把deterministic policy定义为：
$$
a=\mu(s,\theta)\doteq\mu(s)
$$
​		可以把$\mu$看作状态空间到动作空间的映射，实际就用神经网络表示。之前我们的$\nabla J$都是针对stochastic policies的。现在我们必须推导在deterministic下的策略梯度是什么。考虑我们的目标函数：
$$
J(\theta)=\mathbb{E}[v_\mu(s)]=\sum_{s\in\mathcal{S}}d_0(s)v_\mu(s)
$$
​		其中$d_0$是概率分布。$d_0$独立于$\mu$。有两种重要的选择$d_0$的情况：第一种是我们只关心某个特殊的状态，我们每次任务都会从这个状态出发，那么就有$d_0(s_0)=1$，这样就有$J(\theta)=v_\mu(s_0)$；另一种情况是$d_0$是behavior policy的stationary distribution，这个和off-policy是有关系的。实际上DPG是天然off-policy的，不用再用重要性采样去转换。原因在于求梯度的结果:
$$
\begin{aligned}
\nabla_{\theta}J(\theta)& =\sum_{s\in\mathcal{S}}\rho_{\mu}(s)\nabla_{\theta}\mu(s)\left(\nabla_{a}q_{\mu}(s,a)\right)|_{a=\mu(s)}  \\
&=\mathbb{E}_{S\sim\rho_{\mu}}\left[\nabla_{\theta}\mu(S)\big(\nabla_{a}q_{\mu}(S,a)\big)|_{a=\mu(S)}\right]
\end{aligned}
$$
​		这里的$\rho_{\mu}$是一个state distribution。这个梯度的具体推导不太好在这里给出，略过。这个公式的意思是，先求q对a的梯度，然后再把所有a全替换成$\mu(S)$。这个梯度和stochastic的梯度是不一样的，重要的区别在于这里不涉及A，也就没有A的梯度，因此这是一个off-policy的算法（因为我们没有要求a要从哪一个策略得到）。

​		有了梯度后，就可以得到优化J的公式，在随机梯度算法下的公式为：
$$
\theta_{t+1}=\theta_{t}+\alpha_{\theta}\nabla_{\theta}\mu(s_{t})\big(\nabla_{a}q_{\mu}(s_{t},a)\big)|_{a=\mu(s_{t})}
$$
​		deterministic actor-critic算法的伪代码如下：

<img src="https://s2.loli.net/2023/08/26/g7OU3mpsFRVi5Ld.png" alt="image-20230825194646443" style="zoom:67%;" />

​		这是一个off-policy的实现，注意$\beta$和$\mu$可以是一致的，我可以给$\mu$加上一些噪音变成$\beta$，让它具备一定的探索性，这时候就变成了on-policy的情况了。当然本质上还是off-policy的。另外一个问题就是，如何选取$q(s,a,w)$？一种方法是早期运用比较广泛的线性函数，即令$q(s,a,w)=\phi^{T}(s,a)w$，$w$是feature vector；另一种方法就是使用神经网络，这样的方法就成了DDPG（deep deterministic policy gradient）。

​		到此我们的RL基础就讲完了，拜拜~
