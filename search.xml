<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>openCV踩坑汇总</title>
      <link href="/posts/4e9580a4.html"/>
      <url>/posts/4e9580a4.html</url>
      
        <content type="html"><![CDATA[<p>​opencv总是在许多地方设定一些奇奇怪怪的反常识的坑，此贴以供记录。本帖主要记录python的opencv；同时对于有些版本引起的问题可能无法涉及。</p><h1 id="图像的shape：并不是宽比高"><a href="#图像的shape：并不是宽比高" class="headerlink" title="图像的shape：并不是宽比高"></a>图像的shape：并不是宽比高</h1><p>​假设我们现在有一张1920*1080的图片，我们用img &#x3D; cv2.imread(“1.png”) 去读取，然后输出image.shape，会得到：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(1080, 1920, 3)</span><br></pre></td></tr></table></figure><p>​<strong>可见img.shape输出的是：高、宽、通道数。但尤其注意，resize的时候，cv2.resize()输入的是(宽，高)而不是(高，宽)！！</strong></p><p>​<strong>另外，openCV的通道排列是B、G、R。</strong></p>]]></content>
      
      
      <categories>
          
          <category> openCV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
            <tag> openCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neural MMO 学习笔记(五)</title>
      <link href="/posts/d4086348.html"/>
      <url>/posts/d4086348.html</url>
      
        <content type="html"><![CDATA[<p>​之前我们是运行了NMMO的示例，并且看到了示例的结果。除去代码层面，还是得要结合一下文档和强化学习的相关学习的。学习NMMO的主要目标，并不是跑一个游戏模型，而是说——如何利用这个框架，实现在<strong>大型</strong>环境下的<strong>高效</strong>学习。</p><p>​官方文档写到：</p><p><img src="https://s2.loli.net/2023/05/09/PSrYtQvomZzRqfH.png" alt="image-20230509132326377"></p><p>​Neural MMO 提供标准环境配置以及脚本模型、使用 CleanRL 预训练的基线以及 WanDB 托管的训练&#x2F;评估日志。什么是CleanRL?它是是一个深度强化学习库，它提供了高质量的单文件实现，具有研究用的功能。github如下： <a href="https://github.com/vwxyzjn/cleanrl">vwxyzjn&#x2F;cleanrl: High-quality single file implementation of Deep Reinforcement Learning algorithms with research-friendly features (PPO, DQN, C51, DDPG, TD3, SAC, PPG) (github.com)</a>。不过CleanRL只包括在线强化学习的实现——什么是在线强化学习？</p><p><img src="https://s2.loli.net/2023/05/09/KbZY5jTpik73sXQ.png" alt="image-20230509133310393"></p><p>​注意这里的用词：<strong>online而不是on-policy</strong>。on&#x2F;off policy （同&#x2F;异策略）强调采样和更新的策略是否相同：</p><p><img src="https://s2.loli.net/2023/05/09/w9KZMWyN58gd3iB.png" alt="image-20230509133920041"></p><p>​说白了就是：看更新Q值用的方法是沿用既定的策略还是用新策略。</p><p>​而on&#x2F;off line是指，比如说我在玩超级马里奥，在线学习是我边玩边学，第一关没过我就学到了很多；而离线学习是我死了或者过了第一关后，我开始回顾并且更新策略。它强调更新权重的时机。</p><p>​<strong>（所以我强烈推荐写强化学习博客的同学在写在线学习的时候写清楚是on-policy还是online）</strong></p>]]></content>
      
      
      <categories>
          
          <category> Neural MMO </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Newral MMO </tag>
            
            <tag> 强化学习 </tag>
            
            <tag> 多智能体 </tag>
            
            <tag> 在线学习 离线学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习(二)</title>
      <link href="/posts/aa0fa061.html"/>
      <url>/posts/aa0fa061.html</url>
      
        <content type="html"><![CDATA[<p>​上文中我们对强化学习做了一些概述，但都只是对于单智能体而言。本文主要介绍多智能体。By the way，多智能体强化学习的学习需要有博弈论的相关知识。从决策者的角度，多智能体系统分为单决策者和多决策者。</p><h1 id="单决策者"><a href="#单决策者" class="headerlink" title="单决策者"></a>单决策者</h1><p>​我们考虑环境里有多个行动者，但只有一个决策者。此时，决策者告诉其他智能体该做什么。我们假设“智能体会简单地执行被告知的事情”——这个假设称作仁者假设（benevolent agent assumption）。但是即便在仁者假设下，多个行动者的行动也要考虑<strong>同步</strong>关系。对行动者A和B，他们可能会同时行动，也可能同一时刻某动作必须互斥，也可能必须满足某种先后关系（序贯动作）。</p><p>​当然，如果每个主体的信息都能集中起来，然后给总体规划进行执行，那么一个多体问题也可以看作一个单智能体问题。当这种集中是无法实现时，我们就面临分散规划（decentralized planning）问题。</p><h1 id="多决策者"><a href="#多决策者" class="headerlink" title="多决策者"></a>多决策者</h1><p>​不同于单决策者情况——试想我们环境中其他行动者也是决策者，它们中每一个都有着自己的偏好，都会自己规划、自己选择。我们称之为对应体（counterpart）。此时可分为两个情况：</p><p>1.虽然有多个决策者，但是共同目标是相同的。此时的问题主要是协调问题（coordination problem）：每个人都要朝着一个方向努力，不能彼此破坏规划。</p><p>2.每个决策者都有自己的偏好，即使这些偏好截然相反。</p><p>​显然，这就意味着我们要进入博弈论的领域了。博弈论是多智能体系统决策的理论基础。博弈论可以通过两种方式用于人工智能：</p><p>1.智能体设计。智能体用博弈论分析可能的决策，假设其他智能体采取理性行动时计算每个决策的期望效用。</p><p>2.机制设计。 我们可以定义环境规则，即智能体必须参与的博弈，使得每个智能体都采用自身效用最好的方案时，最大化集体利益。</p><p>​博弈论为我们提供了一系列不同的模型，其中每个模型都有自己的一套基本假设。其中最重要的区别在于我们是否应该将其视为合作博弈。博弈分两种：</p><p>· 合作博弈。智能体之间存在一个具有约束力的协约，保证智能体之间稳健合作。</p><p>· 非合作博弈。 非合作并不是说只有竞争没有合作。它只是说，没有中心协约来约束所有智能体合作，智能体可以自行决定合作，只要最大化自己利益就行。</p><p>​</p>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> 智能体 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neural MMO 学习笔记（四）</title>
      <link href="/posts/a719dae3.html"/>
      <url>/posts/a719dae3.html</url>
      
        <content type="html"><![CDATA[<p>​写这篇笔记的时候也在学习强化学习相关知识，移步：<a href="https://www.ameshiro77.cn/posts/34960eab.html">强化学习（一） | 雨白的博客小屋 (ameshiro77.cn)</a></p><p>​上文说到我们希望运行出一个游戏界面，而之后我发现是经典读东西读不全的问题，官方文档给出的提示是让我们运行demos.minimal后去运行client里的可执行文件，而非用浏览器打开对应端口（虽然不知道为什么引入了three.js库（做图形学用过这个），但他的示例是用unity做的）。教程还贴心的告诉我们要运行符合系统的可执行文件，我之前之所以运行不了是因为运行了上次在windows下用的.exe。本文先按照官方文档的TUTORIALS的教程来。</p><p>​顺便解释个名词：<code>scripted_agent</code>：动作由自定义脚本所控制的智能体。</p><h1 id="Minimal-Example"><a href="#Minimal-Example" class="headerlink" title="Minimal Example"></a>Minimal Example</h1><p>​这个示例让我们渲染智能体所在的环境。运行baselines中的minimal后，来到client&#x2F;UnityCilent中,启动右边这个exe。</p><p><img src="https://s2.loli.net/2023/04/25/8Q2iS9LxJPvFyIZ.png" alt="image-20230425123810351"></p><p>​可以看到如下界面：</p><p><img src="https://s2.loli.net/2023/04/25/6BYLCPmA7SOqwaz.png" alt="image-20230425123948469"></p><p>​按tab隐藏下面文字；使用鼠标中键可以调整视角以及进行缩放。按右键可以拖动位置。我们放大视角，选定一个agent，可以对其进行follow：</p><p><img src="https://s2.loli.net/2023/04/25/wMDgTHoQSjNRqea.png" alt="image-20230425124543867"></p><p>​至于下面的commands，目前还没有找到有什么用。另外，这个unity游戏甚至找不到退出键，必须得<strong>alt+F4</strong>才行。。</p><h1 id="Config-Classes"><a href="#Config-Classes" class="headerlink" title="Config Classes"></a>Config Classes</h1><p>​Neural MMO提供了小型、中型、大型预设的基本配置，以及一套游戏系统。通过对预设子类化(大概就是python对子类的定义)来启用游戏系统。比如默认配置是：</p><p><img src="https://s2.loli.net/2023/04/25/ZUVzdIKOx1kcePT.png" alt="image-20230425131538638"></p><p>​而在我们刚才所运行的minimal中，就指定了config:</p><p><img src="https://s2.loli.net/2023/04/25/C86Nj3LMa4VTDY5.png" alt="image-20230425132433003"></p><p>​可以看出用的是中型预设。地图将根据提供的配置在环境实例化时生成，并存放在PATH_MAPS里以供重用。如果主动调整terrain（地形）生成参数，要像上图里设置MAP_FORCE_GENERATION &#x3D; TRUE。（文档写的是FORCE_MAP_GENERATION不知道为啥）</p><p>​当然也可以通过覆盖预设和游戏系统配置参数来自定义地形生成和游戏平衡：</p><p><img src="https://s2.loli.net/2023/04/25/yAGqvSlOdcXgUD3.png" alt="image-20230425133455292"></p><p>​使用config的话，就把自己设定的config当成simulate()的参数就行了。（应该是）</p><p><img src="https://s2.loli.net/2023/04/25/nixbzoEdrDLk4qm.png" alt="image-20230425133554611"></p><p>​源码的simulate()如下：</p><p><img src="https://s2.loli.net/2023/04/25/MOfbEFj1WxCzr7m.png" alt="image-20230425133639963"></p><h1 id="Scripted-API"><a href="#Scripted-API" class="headerlink" title="Scripted API"></a>Scripted API</h1><p>​暂时没看懂这是在干啥。源代码里捏了个LavaAgent，这个Agent喜欢没事就往岩浆里跳，通过nmmo.Agent的子类来实现。这一部分提到了observation和wrapper  class，之后再看看。</p><h1 id="Rewards-amp-Tasks"><a href="#Rewards-amp-Tasks" class="headerlink" title="Rewards &amp; Tasks"></a>Rewards &amp; Tasks</h1><p>​默认情况下，Neural MMO为死亡提供-1的奖励信号，其他行为信号为0.当然我们可以自己覆写奖励。如下：</p><p><img src="https://s2.loli.net/2023/04/25/BSp3ZPeqRgDy1mO.png" alt="image-20230425135312956"></p><p>​这是插入了一个新属性kills。<em>hasattr</em>()函数是用来判断一个对象是否包含对应属性的。这段代码是增加了一个奖励机制：对每一个击败的玩家增加0.1的奖励。比如击杀了两个，就加0.2。当然了，如果只这么加的话，智能体就会总想着杀人而不参与农业了。</p><p>​暂时看到这，看代码的时候还得继续看强化学习的知识。。</p>]]></content>
      
      
      <categories>
          
          <category> Neural MMO </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Newral MMO </tag>
            
            <tag> 强化学习 </tag>
            
            <tag> 多智能体 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>强化学习（一）</title>
      <link href="/posts/34960eab.html"/>
      <url>/posts/34960eab.html</url>
      
        <content type="html"><![CDATA[<p>​本节笔记参考自：《人工智能：现代方法 第4版》。</p><h1 id="什么是智能体-Agent"><a href="#什么是智能体-Agent" class="headerlink" title="什么是智能体 (Agent)?"></a>什么是智能体 (Agent)?</h1><p>​在书上有如下定义：任何通过传感器（sensor）感知环境（environment）并通过执行器（actuator）作用于该环境的事物都可以被视为智能体（agent）。</p><p>​对人类来说，我们的眼睛和耳朵就是传感器；对机器人来说，它的红外探测器就可以看作是传感器。而环境是一个笼统的概念，我们所感知的周围的一切都可以看作环境，甚至整个宇宙。当然，实际问题中我们设计智能体中，所谓环境只关心智能体感知并且影响智能体动作的部分。关系如图所示。</p><p><img src="https://s2.loli.net/2023/04/24/PqIenMvSyB4DHu9.png" alt="image-20230424215050503"></p><p>​术语感知（percept 名词）表示智能体的传感器正在感知的内容。感知序列（percept sequence）表示智能体所感知的一切完整历史。那么一个智能体如何作出动作选择呢？一般来说是根据它的内置知识以及目前为止整个感知序列。数学上讲，我们说智能体的行为由智能体函数（agent function）描述，这个函数的作用是：完成一个 感知序列→动作 的映射。</p><p>​做着正确事情的智能体叫理性智能体（rational agent）。对于智能体表现的评价由性能度量（performance measure）描述。性能度量评估智能体在环境中的行为。给定到目前为止所看到的感知序列，理性智能体的动作是为了最大化性能度量的期望值。</p><p>（后续接着补）</p><h1 id="强化学习概述"><a href="#强化学习概述" class="headerlink" title="强化学习概述"></a>强化学习概述</h1><p>​强化学习（reinforcement learning）是一种无监督学习，智能体与世界进行互动并定期收到反映其表现的奖励（reward）。</p><p>​我们考虑下国际象棋的问题。如果采用监督学习，那么我们要训练出一个模型：$f $(当前局面)&#x3D;下一步下棋走法。很容易想到，这样的数据集太过庞大了，不太可能实现；而且智能体很可能不知道最终的目标是什么，只是被训练的数据牵着鼻子走。因此我们需要强化学习——在这个例子中，我们可以设定获胜的奖励为1，失败的奖励为0，平局的奖励为1&#x2F;2。对于“奖励”这一概念，先引出马尔科夫决策过程（MDP）。<strong>（目前都是概述）</strong></p><h2 id="马尔科夫决策过程"><a href="#马尔科夫决策过程" class="headerlink" title="马尔科夫决策过程"></a>马尔科夫决策过程</h2><h3 id="转移模型"><a href="#转移模型" class="headerlink" title="转移模型"></a>转移模型</h3><p>​转移模型是指给定先前状态值时，最新状态变量的概率分布，即：$ P(X_t|X_{0:t-1}) $  。现在我们面临一个问题：当t不断增长，条件集合  $ X_{0:t-1} $  会无限变大。解决这个问题的方式是马尔科夫假设（Markov assumption）：当前状态只依赖于有限固定数量的过去状态。对于满足这一假设的过程我们称之为马尔科夫过程（Markov process）或者马尔科夫链（Markov chain）。其中最简单的是一阶马尔科夫过程，即当前状态只依赖于前一个状态，即： $P(X_t|X_{0:t-1})&#x3D;P(X_t|X_{t-1})$ 。等式右侧的条件分布就是一阶马尔科夫过程的转移模型。结构如图。</p><p><img src="https://s2.loli.net/2023/04/24/fR2swLomBjcJYSD.png" alt="image-20230424233039700"></p><h3 id="序贯决策问题"><a href="#序贯决策问题" class="headerlink" title="序贯决策问题"></a>序贯决策问题</h3><p>​在这种问题中，智能体的效用取决于一系列决策。转移模型描述了每个状态下每个动作的结果，这些结果是有随机性的，我们写作$ P(s’|s,a) $, 表示在状态s中执行动作a，达到状态s’的概率（也写作 $ T(s,a,s’) $）。我们假设转移是马尔科夫的：从s到s’的概率只取决于s，而不取决于之前的状态。</p><p>​我们规定对于通过动作a从s到s’的每次转移，智能体都会收到一个奖励 $ R(s,a,s’) $ 。奖励可以正也可以负。在一个完全可观测的随机环境下，具有马尔科夫转移模型和加性奖励的序贯决策问题称为<strong>马尔科夫决策过程</strong>（Markov decision process）。动态规划是求解MDP的常用办法。</p><h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><p>​MDP问题的解是什么样子的？没有固定的动作序列可以求解这个问题，因为智能体可能会以一种与目标不同的状态结束。因此问题的解必须告诉智能体在任何可能的状态下应该作什么动作。这种解称为策略，通常用$\pi$来表示一个策略，用$\pi(s)$来表示策略$\pi$对状态s推荐的动作。</p><p>​最优策略（optimal policy）是指能够产生最大期望效用的策略，用$\pi^*$表示。</p><h2 id="从奖励中学习"><a href="#从奖励中学习" class="headerlink" title="从奖励中学习"></a>从奖励中学习</h2><p>​先前我们在MDP中提到了奖励的概念，而事实上强化学习的目标也是相同的:最大化期望奖励总和。在强化学习中，智能体本身处于MDP里，它可能不知道转移模型或者奖励函数，必须采取行动以了解更多的信息。只要为智能体提供正确的奖励信号，强化学习就能提供一种非常通用的构建人工智能系统的方法。</p><p>​强化学习可以根据方法的不同如下分类。</p><h3 id="基于模型的强化学习"><a href="#基于模型的强化学习" class="headerlink" title="基于模型的强化学习"></a>基于模型的强化学习</h3><p>​这些方法中，智能体使用环境的转移模型来帮助解释奖励信号并决定如何行动。模型可能最开始未知，智能体通过观测它的行为带来的影响来学习模型；也可能是已知的。基于模型的强化学习系统通常会学习一个效用函数$U(s)$,，它定义为状态s之后的奖励的综合。</p><h3 id="无模型强化学习"><a href="#无模型强化学习" class="headerlink" title="无模型强化学习"></a>无模型强化学习</h3><p>​这种方法中，智能体不知道环境的转移模型也不会学习这个模型。它会直接学习如何采取行为方式。主要有以下两种形式：</p><h4 id="动作效用函数学习"><a href="#动作效用函数学习" class="headerlink" title="动作效用函数学习"></a>动作效用函数学习</h4><p>​动作效用函数（action-utility function），也称Q函数（Q-function）： $ Q(s,a) $ 是给定状态下采取给定动作a的期望效用，其与效用有关： $ U(s)&#x3D;max_aQ(s,a) $ .Q函数代表了从状态s出发，采取动作a的奖励的总和；智能体通过寻找具有最高Q值的动作来决定在状态s下采取的行动。</p><h4 id="策略搜索"><a href="#策略搜索" class="headerlink" title="策略搜索"></a>策略搜索</h4><p>​智能体学习一个策略$\pi(s)$,即从状态到动作的直接映射。</p><h1 id="多智能体环境"><a href="#多智能体环境" class="headerlink" title="多智能体环境"></a>多智能体环境</h1><p>​很多时候一个智能体必须在包含多个行动者的环境中作出决策，这样的环境称之为多智能体系统（multiagent system）。在这种系统中，智能体要解决多智能体规划问题，其合适的解决方法取决于环境中各智能体间的关系。</p><p>​学多智能体环境，就必须要学习博弈论。多智能体环境在下一篇文章开始介绍。</p>]]></content>
      
      
      <categories>
          
          <category> 强化学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 强化学习 </tag>
            
            <tag> 智能体 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Nerual MMO 学习笔记(三)</title>
      <link href="/posts/3e735cb4.html"/>
      <url>/posts/3e735cb4.html</url>
      
        <content type="html"><![CDATA[<p>​上次说到，我们可能遭遇了关于ray这个库的版本上的问题。所以在这里，先了解一下ray是干什么的，以及相关资料。</p><p>​首先介绍一下ray是干什么的：ray是一个用于分布式计算的多功能框架。ray依赖于高效的c++代码，以及优雅而简单的python API。从机器学习的角度来看，它有两个主要优点：1.它可以用于将流行的工具如Spark和Tensorflow&#x2F;Pytorch捆绑在一个集群中，并为两者之间的数据传输提供优雅的选项。2.当你需要大量的机器学习特别是深度学习的时候，它可以提供分布式计算能力。</p><p>​简单的说，今天的机器学习需要分布式计算。无论是训练网络、调整超参数、服务模型还是处理数据，机器学习都是计算密集型的，如果没有访问集群，速度会非常慢。而ray 是一个流行的分布式 Python 框架，它可以与 PyTorch 配对，以快速扩展机器学习应用。</p><p><img src="https://s2.loli.net/2023/04/17/9nkE8IFxelgi5Ao.png" alt="preview"></p><p>​参考链接：<a href="https://segmentfault.com/a/1190000039283586">程序员 - PyTorch &amp; 分布式框架 Ray ：保姆级入门教程 - 超神经HyperAI - SegmentFault 思否</a></p><p>​那么什么是分布式计算和集群？</p><p>​随着计算技术的发展，有些应用需要非常巨大的计算能力才能完成，如果采用集中式计算，需要耗费相当长的时间来完成。</p><p>​     分布式（distributed）是指在多台不同的服务器中部署<strong>不同</strong>的服务模块，通过远程调用协同工作，对外提供服务。不同服务器中部署不同的功能，通过网络连接起来，组成一个完整的系统。</p><p><img src="https://s2.loli.net/2023/04/17/WAqbORZnhv6zwce.png" alt="image-20230417003653966"></p><p>​集群（cluster）是指在多台不同的服务器中部署<strong>相同</strong>应用或服务模块，构成一个集群，通过<a href="https://so.csdn.net/so/search?q=%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1&spm=1001.2101.3001.7020">负载均衡</a>设备对外提供服务。在不同的服务器中部署相同的功能。</p><p><img src="https://s2.loli.net/2023/04/17/x9NacUy2ZHCmf6G.png" alt="image-20230417003715760"></p><p>（图源<a href="https://blog.csdn.net/qq_49948651/article/details/127003662">什么是分布式和集群？它们有什么区别_DUT_子陌的博客-CSDN博客</a> 侵删）</p><p>​基础概念讲完了，该研究一下ray的相关资料了。上文我们讲到，官方提供的ray版本是0.6.2，但是我们无法pip install。我们直接去pip官网看看0.6.2版本：</p><p><img src="https://s2.loli.net/2023/04/17/mMyFedYTgESP65X.png" alt="image-20230417003940384"></p><p>​好家伙，这下知道为什么下不了了，同时也理解了为什么Neural MMO官方只提供了WSL、Ubuntu、macos三种系统下的安装方式，原来这玩意是真不兼容windows啊。事实上，ray的高版本开始支持windows，但是并不稳定，可能会有很多错误。同时，我在WSL上成功安装了nmmo，但是因为WSL上的anaconda环境有些问题，我还是打算用VM虚拟机继续进一步实现了（其实我不太想用虚拟机，一个是太费空间了，第二个是我的虚拟机总是明明没怎么用、有很大空间却总是说没地方下东西了，第三个是有时候会崩。。）</p><p>​**所以第一个重要结论：请务必用linux系统完成这个项目…**想起以前有个博士学长给我说windows会有一堆问题，没想到这么快就碰的我脑壳晕..</p><p>​打开VM，新开个虚拟机（硬盘开大点），<strong>记得要apt install一下gcc和python3-dev</strong>，否则有些库是安装不了的。然后下载anaconda，安装，接着开个虚拟环境，这些过程都没啥说的。先开个python版本为3.7的虚拟环境，尝试跑一下openai发布的neural mmo那个github的代码：</p><p><a href="https://github.com/openai/neural-mmo">openai&#x2F;neural-mmo: Code for the paper “Neural MMO: A Massively Multiagent Game Environment for Training and Evaluating Intelligent Agents” (github.com)</a></p><p>​由于他需要ray版本是0.6.2，但是这玩意pip install不了，只能去pypi.org找历史发布版本然后安装了。之后运行它说的代码，结果遇到了一个很让人摸不清头脑的问题：</p><p><img src="https://s2.loli.net/2023/04/18/WMqfrB2QAO8DXaY.png" alt="image-20230418004958567"></p><p>​他说找不到这个模块，但是实际上这个embyr是一个不属于任何类型的文件，所以压根不能导入它。。而且，这个github要求python版本要不能超过3.7，因为ray0.6.2只支持3.7及以下；而nmmo官方介绍里的python版本又必须到3.9，否则不能安装nmmo这个package。。简直乱的我头皮发麻。</p><p>​只能先不管这个github了，先看看官方介绍给出的，也就是这些：（记得开个py版本是3.9的新环境）</p><p><img src="https://s2.loli.net/2023/04/18/iuP1qmwhUclKGsZ.png" alt="image-20230418005451549"></p><p>​由于使用了linux版本，我们终于可以愉快地下载nmmo库了。但是用这条指令安装的库巨多：</p><p><img src="https://s2.loli.net/2023/04/18/bmKR8PdlTDuCHev.png" alt="image-20230418005641218"></p><p>​而且占的空间也很大，所以之前会提醒虚拟机稍微开大点。。按照它说的，我们先去搞个wandb api key：<a href="https://wandb.ai/authorize">Weights &amp; Biases (wandb.ai)</a>  然后echo到文件夹里。不得不说他这个官方文档也非常混乱，上面让我clone git下面又让我clone，根本看不懂顺序，而且执行下面的pip install -e -[all]后还把我nmmo给uninstall了就离谱。之后，我们运行python -m demos.minimal：</p><p>​结果这里又报错：</p><p><img src="https://s2.loli.net/2023/04/18/5cY7BMgnzDskEfP.png" alt="image-20230418010127664"></p><p>​大概百度了一下说是循环调用的问题，但是我看了半天源码根本看不出哪里循环调用了，而且源码也不能乱改。。于是大胆猜测是不是版本问题，联想到现在pip list出来的nmmo版本是1.7而最开始pip install nmmo（没加[cleanrl],nmmo[cleanrl]的意思是nmmo增加了对cleanrl的支持，cleanrl是另一个强化学习库）安装的版本是1.6，外加1.6是去年九月发布的而这些github的最后更改时间也是去年九月，于是pip install nmmo&#x3D;&#x3D;1.6.0.6试试，果然不报错了。。最后运行控制台是这个样子：</p><p><img src="https://s2.loli.net/2023/04/18/aNghYerktKwM5dT.png" alt="image-20230418010656681"></p><p>​<strong>不过之前不小心非正常途径把他停止了，导致进程依然在占用，所以再次运行会报错：</strong></p><p><img src="https://s2.loli.net/2023/04/18/lIJorjfTUHhi7Et.png" alt="image-20230418011111547"></p><p>​这个时候需要指令 sudo lsof -i:8080 （8080是开的端口，它这个代码默认是开到了8080）查看pid，然后sudo kill -9 {pid}.如下：</p><p><img src="https://s2.loli.net/2023/04/18/rWzdVbkZHAxqs26.png" alt="image-20230418011225432"></p><p>​再次运行就可以了。然后浏览器localhost:8080 看看是啥东西：</p><p><img src="https://s2.loli.net/2023/04/18/tRvDAyPSs1dMmOl.png" alt="image-20230418011441403"></p><p>​我直接打出一个问号，这是直接把我文件夹放到端口上去了，可是官方文档里的那些游戏界面呢？可能到目前为止我们只完成了库的安装吧，毕竟到现在为止只是user guide部分。。之后继续研究官方文档教程了，希望早日能看到图形化界面（而且我的linux运行不了他们给出的unity做的.exe）… </p>]]></content>
      
      
      <categories>
          
          <category> Neural MMO </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Newral MMO </tag>
            
            <tag> 强化学习 </tag>
            
            <tag> 多智能体 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Nerual MMO 学习笔记(二)</title>
      <link href="/posts/40807c9f.html"/>
      <url>/posts/40807c9f.html</url>
      
        <content type="html"><![CDATA[<p>​首先按照这个官网的挨个来：<a href="https://neuralmmo.github.io/build/html/rst/landing.html">Introduction — Neural MMO v1.6.0 1.6.0 documentation</a> </p><p>​<img src="https://s2.loli.net/2023/04/10/zFsWn2Iw7oh4ker.png" alt="image-20230410233958750"></p><p>​首先是介绍：它说Neural MMO是一个开源且可计算访问（不太明白这俩词合在一起啥意思）的研究平台，一大堆智能体联合生存+探索+战斗持续数小时，以以及完成任务。它在其他环境下这种大规模的研究可能不切实际或不可能进行，也就是NMMO具有大规模的特点。</p><p>​之后它贴了一串代码：</p><p><img src="https://s2.loli.net/2023/04/10/Jk1YAVsf6T2hXzW.png" alt="image-20230410234510556"></p><p>​但显然我们目前没法安装nmmo，暂时没法试这个。但是我去pip库找的时候发现了一个叫neural-mmo的库，可能是nmmo库的历史版本（我猜的），不过暂时还没试。这个Env提供了标准的Petting Zoo API，详见：<a href="https://zhuanlan.zhihu.com/p/375049925">Pettingzoo：类似gym的多Agent强化学习的环境 - 知乎 (zhihu.com)</a></p><p>​之后它说NMMO是把经典MMO开发的技术应用到深度学习，并且介绍了一般的工作流程：</p><p><img src="https://s2.loli.net/2023/04/10/8Juoi5jDTX3atsv.png" alt="image-20230410234919966"></p><p>​也就是生成环境——训练智能体——行为可视化。</p><p>​之后它让我们安装nmmo（虽然目前安不上），以及clone三个库。</p><p><img src="https://s2.loli.net/2023/04/10/yrtGjI1Win8M5Jg.png" alt="image-20230410235041834"></p><p>​但是这三个库暂时压根不知道是干啥的，虽然在client里有一个unity的应用程序可以运行，但打开后的界面看不太懂，因为不管怎么操作都没有反应，就先不管这个了。。</p><p><img src="https://s2.loli.net/2023/04/10/eGdlY7KwfIPuaNg.png" alt="image-20230410235201312"></p><p>​到这里官方文档的User Guide就结束了，是不是感觉一头雾水。<strong>比较坑的是，这里还有一个仓库没有提及</strong>，我也不知道当时怎么找找到的这个github。这是由openai所发布的一个仓库,与上面那三个仓库是不同源的，地址为：<a href="https://github.com/openai/neural-mmo">openai&#x2F;neural-mmo: Code for the paper “Neural MMO: A Massively Multiagent Game Environment for Training and Evaluating Intelligent Agents” (github.com)</a> 下载这个仓库，参考仓库里面的README.md，运行setup.py（前提是下载了scripts&#x2F;setup&#x2F;requirements.txt里面的库（有个坑后面会说））。此时很有可能会报一个错，是说opensimplex函数参数过少，我去pip包官网查阅了一下，发现应该是版本的问题。pip list的话可以看到，版本是0.4，我们改成0.2：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install opensimplex==<span class="number">0.2</span></span><br></pre></td></tr></table></figure><p>​同时，会报一个scripy的错，说是没有imread、imsave。这也是版本问题，但是我直接下载老版本会出问题，解决方法是用另一个库：imageio</p><p><img src="https://s2.loli.net/2023/04/04/2AlxoveERj98Uz7.png" alt="image-20230404155701010"></p><p>​pip install这个库，然后代码改成上图即可。</p><p><img src="https://s2.loli.net/2023/04/04/SEnY72NxryfCVt5.png" alt="image-20230404155728039"></p><p>​到此可以看出setup.py运行完毕，这个命令的作用应该是生成地图，在resource&#x2F;maps&#x2F;procedural里可以看到我们生成的地图。</p><p><img src="https://s2.loli.net/2023/04/10/dKGAaS67PbwOH1D.png" alt="image-20230410235641217"></p><p>​根据README.md里面的Quickstart，我们下载好了NMMO的环境和独立客户端，以进行渲染。注意python版本要3.6+，且拥有pytorch。之后按照它说的：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python Forge.py --render <span class="comment">#Run the environment with rendering on</span></span><br></pre></td></tr></table></figure><p>​结果又报了个错： AttributeError: module ‘ray’ has no attribute ‘PYTHON_MODE’。估计又是版本的问题，因为它说的ray版本是0.6.2，但是pip install根本没办法下载版本这么低的ray了。。之后再查文档改一下吧，麻了</p>]]></content>
      
      
      <categories>
          
          <category> Neural MMO </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Newral MMO </tag>
            
            <tag> 强化学习 </tag>
            
            <tag> 多智能体 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Neural MMO 学习笔记(一)</title>
      <link href="/posts/f9226073.html"/>
      <url>/posts/f9226073.html</url>
      
        <content type="html"><![CDATA[<h1 id="什么是Neural-MMO？"><a href="#什么是Neural-MMO？" class="headerlink" title="什么是Neural MMO？"></a>什么是Neural MMO？</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>​首先，我们可以访问Neural MMO的官网获取信息：<a href="https://openai.com/research/neural-mmo">Neural MMO: A massively multiagent game environment (openai.com)</a></p><p>​<img src="https://s2.loli.net/2023/04/03/vDYLzVCFXaTKiO8.png" alt="image-20230403232817474"></p><p>​对于经常打游戏的同学来说，MMO这个名字应该不会陌生。MMO是指大型多人在线游戏 (Massively Multiplayer Online);而我们的Neural MMO正是一个用于多智能体强化学习的游戏环境。这个环境提供许多智能体（数量可以变化），并且让他们在一个持久、开放的任务中进行工作。这样一个智能体与其他各种生物实体的集合带来了更好的探索、不同的生态位（niche）形成以及更出色的整体竞争力。</p><p>​其代表性论文如下：[<a href="https://arxiv.org/abs/1903.00784">1903.00784] Neural MMO: A Massively Multiagent Game Environment for Training and Evaluating Intelligent Agents (arxiv.org)</a></p><h2 id="Neural-MMO环境与训练"><a href="#Neural-MMO环境与训练" class="headerlink" title="Neural MMO环境与训练"></a>Neural MMO环境与训练</h2><p>​智能体们可以加入任何环境，对于每一个环境都包含一个自动生成的游戏地图。这个地图叫tile maps，其地图环境包含不能穿越的水和石头、以及可以穿越的森林等等。在每一个时间戳里，每个智能体要观察自身一定半径内的地图，进行一次移动并且发动攻击。站在森林里、水边可以获得食物和水，但是这些资源是有限的；攻击方式有近战、远战、法术等可以选择。这就相当于模拟了一个生态系统，许多智能体在这个系统中进行生存竞争，锻炼出他们的探索能力和整体竞争力。</p><p><img src="https://s2.loli.net/2023/04/04/GH4fkKvq6Fogp2P.png"></p><p>​OpenAI使用NMMO训练了一个AI，奖励award为智能体生存的时间。他们发现：</p><p>· 智能体彼此交互越久，那么他们表现越好。同时存在的智能体越多，探索效果越好。</p><p>· 增加智能体种群规模可以促使他们分散到地图不同区域；大环境的训练效果优于小环境。</p><h2 id="其他的多智能体强化学习环境"><a href="#其他的多智能体强化学习环境" class="headerlink" title="其他的多智能体强化学习环境"></a>其他的多智能体强化学习环境</h2><p>​本部分参考自：<a href="https://zhuanlan.zhihu.com/p/592544395">常见多智能体强化学习仿真环境介绍【一】 - 知乎 (zhihu.com)</a></p><h1 id="Neural-MMO安装"><a href="#Neural-MMO安装" class="headerlink" title="Neural MMO安装"></a>Neural MMO安装</h1><p>​首先进入NMMO的introduction界面：<a href="https://neuralmmo.github.io/build/html/rst/landing.html#icon-installation">Introduction — Neural MMO v1.6.0 1.6.0 documentation</a> 可以看到它给我们的提示方法是：</p><p><img src="https://s2.loli.net/2023/04/04/lTEDKBeCUoN84gR.png" alt="image-20230404145651712"></p><p>​我装了很久，但是一直报Twisted相关的错：</p><p><img src="https://s2.loli.net/2023/04/04/rD6bidJqPQuymSE.png" alt="image-20230404145729883"></p><p>​网上也找了很多方法，但是都没有用。鉴于官方安装步骤是以Ubuntu、WSL和MacOS来的，打算先试试WSL（还没成功，暂且请不要跟着做..）。</p><p>​安装WSL Ubuntu20.04，参考链接：<a href="https://blog.csdn.net/tiandiren111/article/details/120984740">win10开启wsl系统，让我们愉快的在windows上使用Linux_DLGG创客DIY的博客-CSDN博客</a></p><p>​这里注意一下，不要随便改安装路径，比如微软商店安到C盘就不要改别的盘，因为这玩意必须安到系统驱动盘。否则初始化的时候你的用户名没法输入的。之后安装anaconda，然后进行一系列操作，但是我还是没成功… 后续见下一篇笔记~</p>]]></content>
      
      
      <categories>
          
          <category> Neural MMO </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Newral MMO </tag>
            
            <tag> 强化学习 </tag>
            
            <tag> 多智能体 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何在Matlab中导入toolbox</title>
      <link href="/posts/a836a3e9.html"/>
      <url>/posts/a836a3e9.html</url>
      
        <content type="html"><![CDATA[<p>​跑老师给的demo的时候遇到了一个问题：无法解析名称 ‘vision.internal.partialSort’</p><p>​显然是缺少对应工具箱导致的，这个工具箱是计算机视觉相关的。但是网上找了好多都是说从外部离线下载，其实在matlab内部就内解决。</p><p><img src="https://s2.loli.net/2023/04/02/xposAMQnzkXwR9r.png" alt="image-20230402195007567"></p><p>​在主页——附加功能 进去 “获取附加功能”，搜索Computer Vision Toolbox然后安装即可，可能要稍微等一会，这个下载需要费点时间。</p><p>​matlab版本：R2022b</p>]]></content>
      
      
      <categories>
          
          <category> Matlab </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Matlab </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Harris角点检测中的公式推导</title>
      <link href="/posts/de1d57b1.html"/>
      <url>/posts/de1d57b1.html</url>
      
        <content type="html"><![CDATA[<p>​在Harris角点检测中，我们要算两个窗口的差。首先，两个点之间的差为：<br>$$<br>S &#x3D;f\left(x_{i}, y_{i}\right)-f\left(x_{i}+\Delta x, y_{i}+\Delta y\right)<br>$$<br>​其中f是像素灰度值。为避免误差，取平方，之后在窗口内求和，得：</p><p><img src="https://s2.loli.net/2023/03/30/NVcB7HOImwJlf84.png" alt="image-20230330204656439"></p><p>​刚拿到这个式子，可能从①到②不知道怎么来的。其实这里是应用了泰勒展开，首先我们知道对于一维函数来说，其泰勒展开式子为：<br>$$<br>f(x+h)&#x3D;f(x)+hf’(x)+\frac{h^2}{2!}f’’(x)+o(h^2)<br>$$<br>​一般来说，展开到两阶就行了。但是对于更普适的情况，令 $f(x)$ 是 $\mathbb{R^n}→R$ 的函数，则其泰勒展开式子为：<br>$$<br>f(\mathbf{x}+\mathbf{h})&#x3D;f(\mathbf{x})+\mathbf{h}^T\nabla f(\mathbf{x})+<br>\frac{\mathbf{h}^T}{2!}\nabla^2 f(\mathbf{x})\mathbf{h}+o(||h||^2)<br>$$<br>​可能会有同学觉得这不是h的转置乘以梯度吗，图里公式怎么是梯度的转置乘以h了？其实这两个是相等的，因为他们结果是标量，标量转置等于自己。</p><p>​对于一阶梯度来说，其结果就是对每个自变量求偏导后组成新向量。而对于二阶梯度，其结果叫海森矩阵，其形式为：</p><p><img src="https://s2.loli.net/2023/03/30/blZ3FrAfxyMUcVY.png" alt="image-20230330210859073"></p><p>​之后的步骤都比较明显了，最后我们得到的M长这样：</p><p><img src="https://s2.loli.net/2023/03/30/FR1GBCkO8E7Nqia.png" alt="image-20230330211136239"></p><p>​M是一个半正定矩阵，证明请读者自行练习。</p>]]></content>
      
      
      <categories>
          
          <category> CV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CV </tag>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Ameshiro的像素RPG小仓库！</title>
      <link href="/posts/b3576427.html"/>
      <url>/posts/b3576427.html</url>
      
        <content type="html"><![CDATA[<p>​借着搭好博客的契机，悄咪咪宣传一波自己古早时期做的简陋粗糙的像素rpg(</p><h2 id="生命线"><a href="#生命线" class="headerlink" title="生命线"></a>生命线</h2><p>​这是我2020年高三网课那段日子里闲得无聊做的，那时是第一次接触rpgmaker，因为那两年接触像素rpg之后就喜欢上了这种类型风格的游戏，也想自己制作一个出来。这个游戏是我第一部作品，做的很粗糙但也很有成就感，共三个ending，顺便一提游戏文件夹藏着攻略哦~</p><p>​链接:<a href="https://pan.baidu.com/s/1cfOuezb0D3Vwoco4yEP59Q">https://pan.baidu.com/s/1cfOuezb0D3Vwoco4yEP59Q</a> 提取码:t9gp</p><p><img src="https://s2.loli.net/2023/03/29/6J954FftqTYP2Dv.png" alt="image-20230329231914198"></p><p><img src="https://s2.loli.net/2023/03/29/xUuGEXHPtBhNMAq.png" alt="image-20230329232038992"></p><p>​当初做这游戏的时候大晚上想剧情，测试，自己把自己吓得不轻233，不过游戏并没有jump scare，基本是微恐怖~</p><h2 id="橙旅"><a href="#橙旅" class="headerlink" title="橙旅"></a>橙旅</h2><p>​为了给我最喜欢的p主——orangestar庆祝生日，特意做了这部作品，当初本来是做给orangestar粉丝群的大家玩的，角色也是取自各位群友，但这么久过去了也该分享出来啦~</p><p>​链接:<a href="https://pan.baidu.com/s/1GcmoEA6ostK_3qGxPBo5sA">https://pan.baidu.com/s/1GcmoEA6ostK_3qGxPBo5sA</a> 提取码:1gfc</p><p>​这是我第二部作品，结局是五个，同样的文件夹里藏着攻略，流程不长，希望大家喜欢~</p><p><img src="https://s2.loli.net/2023/03/29/j2KQJkENHgvtMT9.png" alt="image-20230329232421446"></p><p>​<img src="https://s2.loli.net/2023/03/29/t3DVghLniaXwIWj.png" alt="image-20230329232449691"></p><p>​这一次的游戏中，我也同样注重细节的处理，也选取了自己最喜欢的BGM，制作游戏的时候想解谜和剧情想的脑阔痛，但我确实这方面灵感和水平不行，还请多多包涵~</p><h2 id="五色推箱子"><a href="#五色推箱子" class="headerlink" title="五色推箱子"></a>五色推箱子</h2><p>​这是我大一的时候，专业有个东西叫特色评测，我就闲来无事拿rpgmaker做了个推箱子，基本没有剧情，只有四关，前两关白给，第四关很难（虽然我是抄的现有的关卡）。</p><p>​链接: <a href="https://pan.baidu.com/s/1GVAF_1Xk5QLnO__CPsBHXg">https://pan.baidu.com/s/1GVAF_1Xk5QLnO__CPsBHXg</a>  提取码q22a</p><p><img src="https://s2.loli.net/2023/03/29/3OkUgKwnqa6erlB.png" alt="image-20230329232712064"></p><p><img src="https://s2.loli.net/2023/03/29/pZVnTbXRvJ27xhI.png" alt="image-20230329232828503"></p><p>​别问为什么五色，因为我太菜了不会五个一样的颜色，因为VX ACE好像不能判断物体坐标位置！！</p><h2 id="蜜柑大陆"><a href="#蜜柑大陆" class="headerlink" title="蜜柑大陆"></a>蜜柑大陆</h2><p>​制作中，敬请期待……（虽然咕咕咕了一年多了）</p><p><img src="https://s2.loli.net/2023/03/29/7cY2RZDMwp6OINu.png" alt="image-20230329232952051"></p><p><img src="https://s2.loli.net/2023/03/29/QuqoBD4G7s9y3pI.png" alt="image-20230329233008011"></p><p>​这部作品我加入了很多大佬的脚本，也加入了打怪的元素，目前剧本未定，地图暂无，只想好了大致区域名称位置以及十二个职业的名称，技能只设定好了三个职业的（真的麻烦这玩意T^T）</p><hr><p>​  我说怎么老是部署不上去，我一直以为是要等好久，结果发现挂着梯子部署就行了。。</p>]]></content>
      
      
      <categories>
          
          <category> 游戏 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 游戏 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于(Ax-b)二范数平方为凸函数的证明</title>
      <link href="/posts/b560505.html"/>
      <url>/posts/b560505.html</url>
      
        <content type="html"><![CDATA[<p>​上文我们求出了<br>$$<br>\dfrac{\mathrm{d}||\mathbf{Ax}-\mathbf{b}||^2_2}{\mathrm{d}\mathbf{x}} &#x3D; 2\mathbf{A}^T\mathbf{A}\mathbf{x} - 2\mathbf{A}^T\mathbf{b}<br>$$<br>​回想高数中，证明凸函数就是看二阶导数（如果存在）大于等于零。映射到矩阵，就是看其海森矩阵是否半正定。</p><p>​海森矩阵也就是对这个结果进一步求导，结果显然是 $\mathbf{2A}^T\mathbf{A}$ , 而如果一个矩阵能够被写成  $\mathbf{A}^T\mathbf{A}$ 或  $\mathbf{A}\mathbf{A}^T$ ,那么他就是半正定的。</p>]]></content>
      
      
      <categories>
          
          <category> CV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数学 </tag>
            
            <tag> 计算机视觉 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于(Ax-b)二范数平方的求导</title>
      <link href="/posts/3e5cc5a1.html"/>
      <url>/posts/3e5cc5a1.html</url>
      
        <content type="html"><![CDATA[<h2 id="其实是一个基础问题"><a href="#其实是一个基础问题" class="headerlink" title="其实是一个基础问题"></a>其实是一个基础问题</h2><p>​在CV的全景拼接中，遇到个一个向量微分的问题，也就是关于矩阵的最小二乘，虽然第一次做的时候茫然无措，但它实际上就是一个标量对矢量的求导。</p><p>​现在要求这个式子：<br>$$<br>\dfrac{\mathrm{d}||\mathbf{Ax}-\mathbf{b}||^2_2}{\mathrm{d}\mathbf{x}}<br>$$<br>​虽然分子看着吓人，但他其实就是个由  $ \mathrm x$ 向量中的各个元素组合成的多项式的值。对于二范数的平方，其值就等于：<br>$$<br>||\mathbf{Ax}-\mathbf{b}||^2_2&#x3D;(\mathbf{Ax}-\mathbf{b})^{T}\hspace{+0.1em} (\mathbf{Ax}-\mathbf{b})<br>$$<br>​运用转置相关知识：<br>$$<br>(\mathbf{Ax}-\mathbf{b})^{T}&#x3D;\mathbf{x}^T\mathbf{A}^T-\mathbf{b}^T<br>$$<br>​之后矩阵乘法：<br>$$<br>||\mathbf{Ax}-\mathbf{b}||^2_2&#x3D;\mathbf{x}^T\mathbf{A}^T\mathbf{Ax}-\mathbf{x}^T\mathbf{A}^T\mathbf{b}-\mathbf{b}^T\mathbf{Ax}+\mathbf{b}^T\mathbf{b}<br>$$<br>​    注意到，$\mathbf{x}^T\mathbf{A}^T\mathbf{b}&#x3D;\mathbf{b}^T\mathbf{Ax}$ , 因为它们最后的结果是一个数，数的转置就是他自己，所以这个等式成立。这样，等式就变成了：<br>$$<br>||\mathbf{Ax}-\mathbf{b}||^2_2&#x3D;\mathbf{x}^T\mathbf{A}^T\mathbf{Ax}-2\mathbf{x}^T\mathbf{A}^T\mathbf{b}+\mathbf{b}^T\mathbf{b}<br>$$<br>​    这时需要引出两条特别重要的结论：<br>$$<br>① \hspace{1.em}\dfrac{\mathrm{d}(\mathbf{x}^T\mathbf{Ax})}{\mathrm{d}\mathbf{x}} &#x3D; (\mathbf{A}+\mathbf{A}^T )\hspace{0.2em}\mathbf{x}<br>$$</p><p>$$<br>\hspace{1.1em}②\hspace{1em} \dfrac{\mathrm{d}(\mathbf{a}^T\mathbf{x})}{\mathrm{d}\mathbf{x}} &#x3D; \dfrac{\mathrm{d}(\mathbf{x}^T\mathbf{a})}{\mathrm{d}\mathbf{x}}&#x3D; \mathbf{a}<br>$$</p><p>​</p><p>​两个的证明其实不难，因为分子都是标量，本质就是把这个标量用 $\mathbf{x}$ 中的分量表示出来，然后利用标量对矢量求导的定义就行了。我们把这两条规则代入：<br>$$<br>\dfrac{\mathrm{d}(\mathbf{x}^T\mathbf{A}^T\mathbf{Ax})}{\mathrm{d}\mathbf{x}} &#x3D; (\mathbf{A}^T\mathbf{A}+(\mathbf{A}^T\mathbf{A})^T)\hspace{0.2em}\mathbf{x} &#x3D; 2\mathbf{A}^T\mathbf{A}\mathbf{x}<br>$$</p><p>$$<br>\dfrac{\mathrm{d}(2\mathbf{x}^T\mathbf{A}^T\mathbf{b})}{\mathrm{d}\mathbf{x}}&#x3D;2 \dfrac{\mathrm{d}(\mathbf{b}^T\mathbf{A}\mathbf{x})}{\mathrm{d}\mathbf{x}} &#x3D; 2\mathbf{A}^T\mathbf{b}<br>$$</p><p>​由于 $\mathbf{b}^T\mathbf{b}$ 与 $\mathbf{x}$ 是无关的，所以求导为零。这样，就得到了：<br>$$<br>\dfrac{\mathrm{d}||\mathbf{Ax}-\mathbf{b}||^2_2}{\mathrm{d}\mathbf{x}} &#x3D; 2\mathbf{A}^T\mathbf{A}\mathbf{x} - 2\mathbf{A}^T\mathbf{b}<br>$$</p><p>​由于 $ ||\mathbf{Ax}-\mathbf{b}||^2_2 $ 是凸函数（证明海森矩阵半正定，一阶导数我们已经求出来了），所以极值点就是极小值点，也就是说令导数为零，最终得到的结果就是我们要找的结果：</p><p>$$<br>\mathbf{x}&#x3D;(\mathbf{A}^T\mathbf{A})^{-1}\mathbf{A}^T\mathbf{b}<br>$$</p><hr><p>​由于两年前学的线代几乎全忘光了，也没有接触过向量函数的微分，所以第一次见到它的时候就有点懵逼。不过感谢一些大佬的帮忙，让我对此有点入门；同时也感谢amekui教我怎么用latex写公式，这篇博客是我第一次接触latex，全篇都是自己手打出来的，感觉好累T^T (hexo竟然不能\newline 害的我又改了一下)</p>]]></content>
      
      
      <categories>
          
          <category> CV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数学 </tag>
            
            <tag> 计算机视觉 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>终于搭完框架了..</title>
      <link href="/posts/40443a89.html"/>
      <url>/posts/40443a89.html</url>
      
        <content type="html"><![CDATA[<p>好累啊，搭了两天，踩了一堆坑，最终也是参阅着各位dalao的文章和博客搭好了，买域名花了35，该考虑怎么装饰了。。。</p>]]></content>
      
      
      
    </entry>
    
    
  
  
    
    
    <entry>
      <title></title>
      <link href="/404.html"/>
      <url>/404.html</url>
      
        <content type="html"><![CDATA[<p>title: 404<br>date: 2019-08-5 16:41:10<br>type: “404”<br>layout: “404”<br>description: “Oops～，我崩溃了！找不到你想要的页面 :(“</p>]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>时间线</title>
      <link href="/archives/index.html"/>
      <url>/archives/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>tags</title>
      <link href="/tags/index.html"/>
      <url>/tags/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>关于</title>
      <link href="/about/index.html"/>
      <url>/about/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>友链</title>
      <link href="/link/index.html"/>
      <url>/link/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
    
    <entry>
      <title>分类</title>
      <link href="/categories/index.html"/>
      <url>/categories/index.html</url>
      
        <content type="html"><![CDATA[]]></content>
      
    </entry>
    
    
  
</search>
