---
uuid: cc670190-8b72-11ef-b265-67a5560ca509
title: 论文阅读-分布式强化学习应用于车联边缘计算中的联邦学习
author: Ameshiro
top: false
cover: 'https://s2.loli.net/2023/04/03/pvexKZFJ94oGbu8.jpg'
toc: true
mathjax: true
copyright_author: Ameshiro
hidden: false
tags:
  - 强化学习
  - 边缘计算
  - 生成式AI
  - 论文阅读
categories:
  - AI无线通信
abbrlink: 7d2bfb36
date: 2024-10-14 11:57:44
updated:
description:
img:
top_img:
password:
summary:
copyright:
copyright_author_href:
copyright_url:
copyright_info:
aplayer:
highlight_shrink:
aside:
---

论文地址：[Paper tables with annotated results for Distributed Deep Reinforcement Learning Based Gradient Quantization for Federated Learning Enabled Vehicle Edge Computing | Papers With Code](https://paperswithcode.com/paper/distributed-deep-reinforcement-learning-based/review/#:~:text=Distributed Deep Reinforcement Learning Based Gradient Quantization for,of vehicles' local models instead of local data.)，SCI 一区，IF8.2。

# 摘要

​		本文的背景是联邦学习与车联边缘计算（vehicle edge computing, VEC）。联邦学习共享车辆本地模型的梯度而非数据本身，这样的意义是可以保护数据隐私；但模型梯度通常很大，传输延迟会很高，因此有人提出梯度量化的方式，即压缩梯度、减小比特数（即量化等级）来传递梯度。影响模型精确度和训练时间的指标主要在于：量化等级和阈值的选择，因为二者决定了量化的误差（quantization error，QE）。因此，**总训练时间和量化误差QE就成了FL驱动的VEC中的关键指标，也即强化学习方法需要最大化的奖励**。此外本文也建模了一种挑战，即**时变信道条件**。

# 简介

​	总之就是说，车辆上传感器很多，所以产生数据很大（1G/1s）。因此车辆需要强大的算力来处理数据，但显然这是有限的，因此就用一个基站来连接边缘服务器吗，收集利用这些数据（的梯度，以防隐私泄露）进行训练。即，在每一轮训练中，所有汽车先用本地数据训练本地模型，然后BS收集本地模型的梯度并聚合成全局模型，再之后边缘服务器把全局模型发送给车辆进行下一轮训练。

​	由于梯度大，我们需要采用梯度量化来减少传输梯度需要的时延，减小训练时间；但量化水平太低又可能增加收敛时间，以及让模型不准确。此外，由于车辆是在路上动的，所以带来时变路径损耗。

​		DRL可以解决复杂环境里的这些问题。为什么选择分布式DRL呢？现有一些方法是集中式DRL处理的，这种框架流量负载高、且因为车辆移动性高所以BS难以收集准确的CSI。而在分布式DRL中，每辆车独立决策，无需发送所有信息到中央结点，减少了通信开销。

# 系统建模

​		AI通信最重要的地方之一就是要构建一个系统模型场景，也就是做出一个environment simulation。来看看这篇文章是怎么建模的：

![image-系统模型](https://cdn.jsdelivr.net/gh/Ameshiro77/BlogPicture/pic/image-20241017001448451.png)

​			考虑有N个车辆，记作$V_N$。其中N个车辆的速度服从一个高斯分布，且匀速不变。车行驶时间分为一系列time slots。每辆车配备一个MEC处理器可以处理本地数据，并有自己的本地数据集$D_n=\{x_i,y_i\}$。由于考虑到传输时间和梯度计算时间，有些车可能在有一些round中时间不够参与，因此要在每个round选择K个车。然后，每个选定的车从BS下载全局模型，这些车以均匀的方式在数据集D中随机选样本算梯度；这些车根据局部观测做出决策，确定量化水平，然后把这些算出来的梯度量化并用OFDMA上传到BS。

​		总之，这个系统环境输入V个车，输出优化后的全局模型w。环境的流程是：

​		1.初始化全局模型$w^0_g$(0表示round数)；

​		2.在每一个round中，每个车下载全局模型；

​		3.每个车获得其移动信息x和v，即当前所处位置和速度，然后据此算出在BS覆盖范围内的停留时间（residence time）；然后接受一个全局消息，即一个round的时间和决策阈值；

​		4.每个车根据本地与全局模型的相似度和停留时间，算出其效用utility；

​		5.每个车看看这个效用是不是超过决策阈值，超过了就选中，否则不选；

​		经过2~5,得到了选中的集合$V_k$，对其中每个车，进行以下操作：

​		1.先算用全局模型下，本地数据集D得到的损失；

​		2.根据这个损失值，得到本地的梯度；

​		3.量化本地的梯度。

​		然后，每个车把量化的本地梯度和本地损失函数送到BS。BS聚合这些梯度，然后更新全局梯度、计算全局损失函数：
$$
\mathbf{w}_g^{r+1}=\mathbf{w}_g^r-\frac{\eta_r}{K}\sum_{k=1}^K\mathcal{Q}(\mathbf{g}_k^r),F(\mathbf{w}_g^r)=\frac{1}{K}\sum_{k=1}^KF(\mathbf{w}_k^r).
$$
​		当满足收敛要求（即达到预期精度或预定损失值）时，整个训练过程完成了。

# 问题建模

​		在建模和系统模型后，就要对问题进行建模，即确定需要优化的目标以及约束。这些都是DRL框架中设计reward需要的。

## 训练时间

​		FL中 每一轮第K个车的训练时间包括：本地训练计算时间、上传时间、全局聚合梯度时间（很快可以忽略）、下载时间（很快可以忽略）。

​		训练计算时间：由车辆更新样本需要的cycles数和CPU的频率决定。

​		上传时间：由量化后需要上传的bits数和传输速率决定。这里的传输速率和Shannon theorem有关：
$$
R_{k}^{r}=\frac BW\log_2(1+\gamma_k^r),\\\gamma_{k}^{r}=\frac{p_{k}h_{k}^{r}(d_{k}^{r})^{-\alpha}}{\sigma^{2}},
$$
​		得到这些后，就可以得到$R_\lambda$轮（叫做最小收敛轮数）下的总的需要的训练时间。但是这个值的推导设计相当复杂的公式运算，看不懂，贴出来结果仅供参考：

![image-公式](https://cdn.jsdelivr.net/gh/Ameshiro77/BlogPicture/pic/image-20241017131311535.png)	

## 量化误差

​		由模型大小、第k车的量化等级和第r轮的本地梯度有关。定义为：
$$
E_k^r=\frac{\sqrt{d}}{q_k^r}\|\mathbf{g}_k^r\|^2.
$$

## 优化问题建模

​		我们现在阅读的论文都建模出了一个优化问题，且这些问题都是非凸的、不能用传统方法解决的。比如这篇论文建模问题为：
$$
\begin{aligned}
\operatorname*{min}_{q_{k}^{r}}& \omega_{1}\cdot T_{k}^{r,\mathrm{total}}+\omega_{2}\cdot E_{k}^{r}, \\
\text{s.t.}& q_{k}^{r}\geq2,and,q_{k}^{r}\in\mathbb{Z}^{+},\forall k\in\mathcal{K}^{r} \\
&1\leq K\leq N,
\end{aligned}
$$
​		这是一个非线性约束的整形优化问题，虽然用branch and bround method可以提供近似解但是复杂度很高。同时，这些传统方法是基于CSI已知的假设，而实际中获取所有CSI事不可能的。这也是为什么用DRL框架处理问题：与动态信道环境交互，并最大化效用。由于集中式DRL框架导致负载明显高，且车辆移动性高使得BS难以收集准确的CSI，因此本文采用分布式DRL方案。 

# 分布式DRL解法

​		在最后，根据建模出的优化问题，决定DRL框架的state、action、reward，然后采用合适的算法。在每一个round的时间步t，每个被选择的Vk观测到局部状态并根据策略做出动作。注意，这里的Vk是之前每轮筛选出的车辆集合。

## 状态

​		状态通常来源于system model、问题约束。本文状态由下面项组成：Vk车与BS的距离$d$（因为影响Vk的移动性）；BS检测出的Vk在t-1时刻（反映出信道不确定）的SNR,$\gamma$；t时刻的量化等级$q$，因为它影响了传输比特数、最小收敛轮和QE。记作：
$$
s_{k,t} = [\gamma_{k,t-1}，d_{k,t},q_{k,t}]
$$

## 动作

​		即第k个车在t时刻选择的量化等级，是一个[2，...，10]的集合。

## 奖励

​		就是优化目标取负（因为优化目标是最小化，所以reward要最大化），然后算一个episode里的累积期望。

在强化学习框架上，本文选择了DDQN算法，其框架如下：

![image-framework](https://cdn.jsdelivr.net/gh/Ameshiro77/BlogPicture/pic/image-20241017161535922.png)

P.S. : DDQN的critic和actor可以看作是一起的，因为它是直接根据最大Q值选动作，而不是输出一个分布。

# 总结

​		AI+无线通信决策，大致分为以下阶段：1.选择问题背景；2.根据通信背景进行system modeling；3.找出建模场景中，需要优化的目标和约束；4.利用DRL框架，找到建模的system中的state和action，然后根据优化目标和约束设计reward；5.设计合适的DRL算法。

​		这篇文章用的是动态车联网环境下的。可以考虑一个静态环境：

![image-SIMPLE IDEA](https://cdn.jsdelivr.net/gh/Ameshiro77/BlogPicture/pic/image-20241017173056664.png)

note: 信道共享；信道衰落、噪声与去噪；优化目标的确定
