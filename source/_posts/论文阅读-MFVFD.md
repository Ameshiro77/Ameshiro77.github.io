---
uuid: 2d6f5cd0-530c-11ee-a5e4-eb81205ea9f4
title: MFVFD 论文阅读
author: Ameshiro
top: false
cover: 'https://s2.loli.net/2023/04/03/pvexKZFJ94oGbu8.jpg'
toc: true
mathjax: true
copyright_author: Ameshiro
hidden: false
img: 'https://s2.loli.net/2023/09/14/z1khxYHdGRurpXv.jpg'
tags:
  - 论文阅读
categories:
  - 强化学习
abbrlink: dbeda41
date: 2023-09-14 22:37:06
updated:
description:
top_img:
password:
summary:
copyright:
copyright_author_href:
copyright_url:
copyright_info:
aplayer:
highlight_shrink:
aside:
---

这篇2021年的论文是把MF和值分解结合起来。值分解知识参考：

[[伏羲讲堂\]多智能体强化学习中的值函数分解——VDN、QMIX、QTRAN - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/203164554)

----

# 摘要

​		在CTDE(centralized training and decentralized execution，集中训练分布执行)的流行范式下，VFD(Value function decomposition，值函数分解)方法促进了MARL的发展。但是呢，现在的根据组(group)的VFD只是用来解决合作问题。这篇论文提出，利用个体(individual)值分解，它们提出了MFVFD，一种利用平均场理论的多智能体Q学习方法。

# 介绍

​		在复杂的、非平稳的(non-stationary)的MARL环境下，要让决策照着全局最优方向走的话，就需要每个智能体感知环境和其他智能体。但是，要追求有效的MARL的话，有两个主要的挑战：智能体观测的局部性(不可能观测到全局)、可伸缩性的限制（我觉得还有不稳定性，比如一个智能体一直采用一样的动作，但获得奖励可能差别很大，因为奖励取决于联合动作）。特别地，观测的局部性虽然能提高决策效率，但是会限制智能体找自己的最优动作。另一方面，如果你要用环境的所有信息去优化的话，状态空间可能会巨大无比，会指数上升，因而带来可伸缩性限制。为了解决这俩挑战，有两种方向：一种是研究人员想解决观测的局部性，为此提出了**CTDE范式**，智能体的策略根据获得的全局信息集中训练，但是执行是依赖局部观测执行。 为了依靠CTDE解决可伸缩性限制，一些依赖于IGM(Individual-Global-Max)原则的值分解算法（诸如QMIX,VDN,QTRAN）被提出；IGM强调最优联合行为应该是个体最优行动的集合。显然，这种将团队的价值函数分解为团队成员的价值函数的方法，将这些方法限制在了cooperate MARL的范围。

​		另一种主要的方法是，采用平均场理论，解决随机博弈中的可伸缩性问题；平均场理论把智能体群体的交互估计为一个智能体和群体作用平均的交互。这样，联合状态-动作（state-action）就可以被状态-动作的分布替代，来减少空间维度。但是，这种方法**只适用于所有代理都属于同一类型**。2020年，Subramanan用K-means算法将其扩展为不同类型，但智能体数量增加时这种方法就无效了。而且，这俩方法并没有分散执行，它们的策略依赖于邻居行为的获取和估计，这在局限观测下很难起作用。

​		于是，本文为了使得MARL在局部观测条件下，适用于合作、竞争或混合任务，提出了MFVFD。在该方法中，每个智能体自身的效用都被集体平均效用影响；因此，每个智能体个体的标准Q函数（基于全局状态和联合行动）就可以转变成**它自身局部Q函数**（基于它自身的局部观测和行为）与**它的MFQ**（基于它邻居的观测和动作分布）的和。也就是说，这种方式是从智能体个体角度分解联合Q函数，而不是按通常地从集体角度分解。这就使得它还可以用于非合作任务。论文说，MFVFD是第一个在局部观测约束下，有效地在个体水平实现高可伸缩性的多智能体Q学习算法。

​		论文也提出它们在MAgent上试过了，效果很好，但是代码暂时找不到~

# 背景

​		论文主要提及三个背景：随机博弈，其求解与CTDE&VFD。随机博弈可以表示为$<\mathcal{S},\mathcal{A}^{1},\ldots,\mathcal{A}^{N},r^{1},\ldots,r^{N},p,\gamma>$，就不再介绍了。纳什均衡是求解随机博弈的基线，传统求解方法复杂度太高，因此近些年提出了平均场强化学习框架，解决随机博弈的可伸缩性。相关研究请见：[MARL周报(6) | 雨白的博客小屋 (ameshiro77.cn)](https://www.ameshiro77.cn/posts/a186851b.html)。但是这要求智能体是同质的，而且不能基于局部观测分布执行。同时，论文指出，在某些方法下，这种方法不能准确估计Q函数，比如，有以下Q函数：
$$
Q^{i}(s,0,1,1)=6,Q^{i}(s,0,2,2)=0,Q^{i}(s,0,1,2)=2
$$
​		我们根据第一个等式，可以得到$Q^{i}(s,0,1,1)=\frac{1}{2}Q^{i}(s,0,1)+\frac{1}{2}Q^{i}(s,0,1)$;因此，我们得到$Q(s,0,1)=6$。显然，我们也能得到$Q(s,0,2)=0$。然而，这样的话，根据平均场公式，我们得到的$Q(s,0,1,2)$应该是$(6+0)/2=3$，不应该是2.因此，学习过程会产生振荡。同时，它依赖于全局状态和观测。

​		在CTDE中，通过集中训练，所有智能体都可以获得所有人的行动-观测和全局状态。这样，智能体就不需要参考联合动作，从而基于局部观测构造个体的Q函数。但是，在CTDE下，需要一个基于全局状态和联合动作的Q，在智能体数量增多时就很难学习了。

​		因此，VFD被提出用来处理这个联合的Q。它的思想可以被描述为：$Q_{jt}(\mathbf{\tau,a})=\sum_{i=1}^{N}Q_{i}(\tau_{i},a_{i})$。其中，$\tau$代表了动作-观测历史。之后，QMIX的提出使得VDN近乎完美；然而，这些方法都是从集体层面分解Q函数，有所限制。

# MFVFD

​		MFVFD的思想是在考虑邻居的影响下，分解每个智能体的原始个体的联合Q函数。正如平均场强化学习所表示的，全局交互可以隐式分解为任意一对代理之间的成对局部交互：
$$
Q^{i}(s,\boldsymbol{a})=\frac{1}{N^{i}}\sum_{k\in\mathcal{N}(i)}Q^{i}(s,a^{i},a^{k}),\quad(1)
$$
​			这里，每个局部交互对的权重都可以看作1/N。考虑不同状态和行为可能效用不同，论文认为每个局部交互对权重也应该不一样，因此这个公式就重新改写成：
$$
Q^{i}(s,\boldsymbol{a})=\sum_{k\in\mathcal{N}(i)}\lambda^{i}(o^{i},o^{k},a^{i},a^{k})Q^{i}(s,a^{i},a^{k}),\quad(2)
$$
​		这里，$\lambda$是权重函数，且显然有$\sum_{k\in\mathcal{N}(i)}{\lambda}^{i}(o^{i},o^{k},a^{i},a^{k})=1$。这样，我们取${\lambda}^{i}(o^{i},o^{1},0,1)=1/3$,之前提到的矛盾就成立了。

​		（待续）
